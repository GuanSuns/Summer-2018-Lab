*************************************
*************************************
*************************************
*** Start Sub-Experiment 0 **********
*************************************
*************************************
*************************************

RUNNING 800 EPISODES

BEGINNING EPISODE: 1

Current Policy Agreement Ratio: 0.000000
Step: 0, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 1, S: (3, 4), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 2, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 4, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
--------------------------------
EPISODE 1 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 7, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.200000
################################

BEGINNING EPISODE: 2

Current Policy Agreement Ratio: 0.200000
Step: 0, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
--------------------------------
EPISODE 2 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 11, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.75}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.200000
################################

BEGINNING EPISODE: 3

Current Policy Agreement Ratio: 0.200000
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 3, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 7, S: (2, 4), A: west, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 10, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 11, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 3 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 25, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 4

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 4 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 29, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 5

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 5 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 31, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.96875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 6

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 6 COMPLETE: RETURN WAS 0.6561

TOTAL STEPS: 36, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.96875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 7

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 7 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 38, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.984375, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 8

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 8 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 45, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.96875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9921875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 9

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 9 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 49, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99609375, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.96875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 10

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 10 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 52, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.998046875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.96875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 11

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 11 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 54, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9990234375, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.96875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 12

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 12 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 58, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99951171875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.984375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99951171875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 13

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (2, 1), A: south, S': (2, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 4, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 13 COMPLETE: RETURN WAS -6.561

TOTAL STEPS: 63, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99951171875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.984375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99951171875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 14

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 14 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 65, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.999755859375, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.984375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 15

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (2, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (2, 3), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 4, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 15 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 72, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9998779296875, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99609375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 16

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 16 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 76, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99993896484375, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.998046875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 17

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 17 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 79, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.999969482421875, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 18

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (4, 1), A: west, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (3, 1), A: north, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 18 COMPLETE: RETURN WAS -5.31441

TOTAL STEPS: 86, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.999969482421875, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 19

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 19 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 94, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999847412109375, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.99609375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.875, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 20

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 20 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 101, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999923706054688, ((2, 4), 'east'): 0.96875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.998046875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 21

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
--------------------------------
EPISODE 21 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 108, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999961853027344, ((2, 4), 'east'): 0.96875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.998046875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 1.000000
################################

##################################
The policy has converged

TOTAL STEPS: 108, EPISODE STEPS: 7
##################################
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999961853027344, ((2, 4), 'east'): 0.96875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.998046875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999923706054688}

##################################

AVERAGE RETURNS FROM START STATE: 5.30131308706


Press Enter to continue*************************************
*************************************
*************************************
*** Start Sub-Experiment 1 **********
*************************************
*************************************
*************************************

RUNNING 800 EPISODES

BEGINNING EPISODE: 1

Current Policy Agreement Ratio: 0.000000
Step: 0, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 2, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 3, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 6, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
--------------------------------
EPISODE 1 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 10, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.200000
################################

BEGINNING EPISODE: 2

Current Policy Agreement Ratio: 0.200000
Step: 0, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
--------------------------------
EPISODE 2 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 14, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.75}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.266667
################################

BEGINNING EPISODE: 3

Current Policy Agreement Ratio: 0.266667
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
--------------------------------
EPISODE 3 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 18, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.333333
################################

BEGINNING EPISODE: 4

Current Policy Agreement Ratio: 0.333333
Step: 0, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 2, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 3, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 5, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 6, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 7, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 8, S: (0, 1), A: south, S': (0, 0), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 9, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 4 COMPLETE: RETURN WAS -3.87420489

TOTAL STEPS: 28, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Action 'south' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 5

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 5 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 33, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0.5, ((2, 3), 'east'): 0.75, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Action 'south' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 6

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 6 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 35, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0.5, ((2, 3), 'east'): 0.75, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Action 'south' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 7

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (0, 1), A: south, S': (0, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 7 COMPLETE: RETURN WAS -7.29

TOTAL STEPS: 39, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.75, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 8

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (4, 1), A: west, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 8 COMPLETE: RETURN WAS 0.729

TOTAL STEPS: 43, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.75, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 9

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 9 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 48, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): -0.03125, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.96875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 10

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 10 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 52, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.484375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.984375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 11

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 11 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 56, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.7421875, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9921875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 12

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 12 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 61, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.87109375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99609375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 13

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 13 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 65, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.935546875, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99609375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.998046875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 14

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (4, 1), A: east, S': (4, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
--------------------------------
EPISODE 14 COMPLETE: RETURN WAS -8.1

TOTAL STEPS: 68, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.935546875, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99609375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.998046875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.600000
################################

BEGINNING EPISODE: 15

Current Policy Agreement Ratio: 0.600000
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
--------------------------------
EPISODE 15 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 70, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9677734375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99609375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.600000
################################

BEGINNING EPISODE: 16

Current Policy Agreement Ratio: 0.600000
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
--------------------------------
EPISODE 16 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 73, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.98388671875, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.998046875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99951171875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.600000
################################

BEGINNING EPISODE: 17

Current Policy Agreement Ratio: 0.600000
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
--------------------------------
EPISODE 17 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 76, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.991943359375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.600000
################################

BEGINNING EPISODE: 18

Current Policy Agreement Ratio: 0.600000
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
--------------------------------
EPISODE 18 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 79, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9959716796875, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.600000
################################

BEGINNING EPISODE: 19

Current Policy Agreement Ratio: 0.600000
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 5, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 7, S: (2, 1), A: south, S': (2, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 8, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 19 COMPLETE: RETURN WAS -4.3046721

TOTAL STEPS: 88, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9959716796875, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 20

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 4, S: (3, 1), A: north, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 20 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 96, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.99798583984375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 21

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 21 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 103, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.998992919921875, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 22

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 22 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 109, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9994964599609375, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 23

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 23 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 113, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9997482299804688, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9921875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 24

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 24 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 118, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9998741149902344, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 25

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 25 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 122, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999370574951172, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 26

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 26 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 124, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999685287475586, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 27

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 27 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 127, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999842643737793, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 28

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 28 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 129, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999921321868896, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999961853027344}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 29

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 29 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 131, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999960660934448, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.96875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999961853027344}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 30

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 30 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 134, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999980330467224, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.96875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999980926513672}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 31

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 31 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 137, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999990165233612, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.96875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999990463256836}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 32

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 32 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 139, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999995082616806, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.984375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999990463256836}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 33

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 33 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 146, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999997541308403, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9921875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.96875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999990463256836}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 34

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 34 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 150, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999998770654202, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.998046875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99993896484375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9921875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.96875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999995231628418}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 35

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 35 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 157, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.96875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999385327101, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.998046875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.99993896484375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.99609375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.984375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999995231628418}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 36

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 36 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 163, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.96875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.999999969266355, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9990234375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.999969482421875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.99609375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.984375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999997615814209}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 37

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 37 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 166, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.96875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999846331775, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9990234375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.999969482421875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.998046875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9921875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999997615814209}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 38

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 38 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 174, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999923165888, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9990234375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.999969482421875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9990234375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.99609375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999997615814209}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 39

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 39 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 177, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999961582944, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9990234375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999847412109375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9990234375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.99609375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999998807907104}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 40

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 40 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 180, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999980791472, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9990234375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999847412109375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.99951171875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.998046875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999998807907104}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 41

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 41 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 183, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999990395736, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9990234375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999847412109375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.999755859375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.9990234375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999998807907104}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 42

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 42 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 187, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999995197868, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99951171875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999923706054688, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.999755859375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.9990234375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999403953552}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 43

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 43 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 194, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.998046875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9921875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999997598934, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99951171875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999923706054688, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9921875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9998779296875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.99951171875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999403953552}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 44

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 44 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 200, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.998046875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9921875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999998799467, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.999755859375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999961853027344, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9921875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9998779296875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.99951171875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999701976776}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 45

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 45 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 203, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.998046875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9921875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999399733, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.999755859375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999980926513672, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9921875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9998779296875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.99951171875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999850988388}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 46

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 46 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 210, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9990234375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99609375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999699867, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.999755859375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999980926513672, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99609375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.99993896484375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.999755859375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999850988388}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 47

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 47 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 216, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.998046875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9990234375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.998046875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999849933, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.999755859375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999980926513672, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.998046875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.999969482421875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.9998779296875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999850988388}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 48

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 48 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 222, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.998046875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9990234375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.998046875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999924967, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9998779296875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9921875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999990463256836, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.998046875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.999969482421875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.9998779296875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999925494194}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 49

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 49 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 227, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.998046875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9990234375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9990234375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999962483, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9998779296875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9921875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999990463256836, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9990234375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999847412109375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.99993896484375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999925494194}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 50

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 50 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 235, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999981242, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9998779296875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9921875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999990463256836, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999923706054688, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.999969482421875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999925494194}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 51

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 51 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 241, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999990621, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999995231628418, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999923706054688, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.999969482421875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999962747097}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 52

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 52 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 243, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.999999999999531, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999995231628418, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999961853027344, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.999969482421875, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999962747097}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 53

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 53 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 246, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999997655, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999995231628418, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999980926513672, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999847412109375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999962747097}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 54

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 54 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 250, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.99609375, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999998828, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999995231628418, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999980926513672, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999847412109375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999981373549}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 55

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 55 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 253, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999414, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999995231628418, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999980926513672, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999847412109375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999990686774}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 56

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 56 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 256, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999707, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999997615814209, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999980926513672, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999847412109375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999995343387}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 57

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 57 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 258, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999853, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999997615814209, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999990463256836, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999847412109375, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999995343387}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 58

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 58 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 265, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999755859375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999927, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999997615814209, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999995231628418, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999923706054688, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999995343387}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 59

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 59 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 269, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999963, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999997615814209, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999997615814209, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999961853027344, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999995343387}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 60

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 60 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 271, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999982, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999997615814209, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999998807907104, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999961853027344, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999995343387}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 61

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 61 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 274, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999991, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.99993896484375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99609375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999997615814209, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999998807907104, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999961853027344, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999997671694}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 62

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 62 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 280, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999996, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.999969482421875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.998046875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999998807907104, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999998807907104, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999961853027344, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999998835847}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 63

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 63 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 286, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999998, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9999847412109375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9990234375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999999403953552, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.984375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999998807907104, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999961853027344, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999999417923}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 64

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 64 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 290, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99993896484375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999999999, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9999847412109375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9990234375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999999403953552, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.984375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999999403953552, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999980926513672, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999999417923}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 65

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 65 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 293, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99993896484375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 1.0, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9999847412109375, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.9990234375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999999701976776, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.984375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999999403953552, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999980926513672, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999999708962}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'west' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 66

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 2, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 3, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 4, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
--------------------------------
EPISODE 66 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 300, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99993896484375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 1.0, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9999923706054688, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99951171875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999999850988388, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999999403953552, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999980926513672, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999999854481}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 1.000000
################################

##################################
The policy has converged

TOTAL STEPS: 300, EPISODE STEPS: 7
##################################
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.999755859375, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99993896484375, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 1.0, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.25, ((2, 3), 'east'): 0.9999923706054688, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.99951171875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.9999999850988388, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0.25, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.9999999403953552, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.9999980926513672, ((4, 1), 'east'): -0.5, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999999999854481}

##################################

AVERAGE RETURNS FROM START STATE: 5.95012583182


Press Enter to continue*************************************
*************************************
*************************************
*** Start Sub-Experiment 2 **********
*************************************
*************************************
*************************************

RUNNING 800 EPISODES

BEGINNING EPISODE: 1

Current Policy Agreement Ratio: 0.000000
Step: 0, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 2, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 3, S: (2, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 4, S: (2, 3), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 5, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 6, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 7, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 8, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 9, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 10, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 11, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 12, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 13, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 14, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 15, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 16, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 17, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
--------------------------------
EPISODE 1 COMPLETE: RETURN WAS 1.66771816997

TOTAL STEPS: 18, EPISODE STEPS: 18
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.333333
################################

BEGINNING EPISODE: 2

Current Policy Agreement Ratio: 0.333333
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 6, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 7, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 2 COMPLETE: RETURN WAS -4.782969

TOTAL STEPS: 26, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 3

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (3, 1), A: south, S': (3, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 3 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 28, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 4

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 4 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 32, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.75, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.75}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 5

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 5 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 36, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 6

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 6 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 39, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 7

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 3, S: (3, 1), A: north, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 7 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 46, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 8

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (2, 1), A: south, S': (2, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 8 COMPLETE: RETURN WAS -7.29

TOTAL STEPS: 50, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 9

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 9 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 53, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.984375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.96875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Action 'east' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 10

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 10 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 57, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 11

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (0, 3), A: east, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (0, 3), A: west, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 11 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 67, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.99609375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 12

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 12 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 72, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.998046875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99609375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9921875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 13

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 13 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 75, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9990234375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.998046875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 14

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 14 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 77, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.99951171875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.998046875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 15

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 15 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 81, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.999755859375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 16

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 16 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 83, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9998779296875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 17

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 17 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 86, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.99993896484375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99951171875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 18

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 18 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 88, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.999969482421875, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 19

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 19 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 96, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999847412109375, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.9375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 20

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 20 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 106, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999923706054688, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0.5, ((4, 1), 'north'): 0.96875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'south' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 21

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 21 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 111, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999961853027344, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0.5, ((4, 1), 'north'): 0.96875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'south' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 22

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 22 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 120, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.96875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999980926513672, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.984375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.96875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 23

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 23 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 128, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999990463256836, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9921875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9921875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.96875, ((3, 1), 'east'): 0.984375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 24

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 24 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 136, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.998046875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9921875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999995231628418, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99609375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.99609375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.984375, ((3, 1), 'east'): 0.9921875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 25

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 25 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 142, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.9990234375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99609375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999997615814209, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.998046875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.998046875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.984375, ((3, 1), 'east'): 0.99609375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 26

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 26 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 148, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.99951171875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.998046875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999998807907104, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9990234375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9990234375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.984375, ((3, 1), 'east'): 0.998046875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 27

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 27 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 154, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.999755859375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9990234375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999403953552, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999755859375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.99951171875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.984375, ((3, 1), 'east'): 0.9990234375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 28

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 28 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 159, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.999755859375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9990234375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999701976776, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.99951171875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.984375, ((3, 1), 'east'): 0.9990234375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 29

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 29 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 161, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.999755859375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9990234375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999850988388, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99951171875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.999755859375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.984375, ((3, 1), 'east'): 0.9990234375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 30

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 30 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 169, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.9998779296875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999925494194, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9998779296875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.99951171875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 31

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 31 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 172, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9998779296875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999962747097, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999755859375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.99993896484375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.999755859375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 32

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 32 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 178, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.99993896484375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999755859375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999981373549, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9998779296875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.999969482421875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.9998779296875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 33

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 33 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 182, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.99993896484375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999755859375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999990686774, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99993896484375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9998779296875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.999969482421875, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.9998779296875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 34

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 34 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 184, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.99993896484375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999755859375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999995343387, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99993896484375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9998779296875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999847412109375, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.9998779296875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 35

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 35 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 187, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.99993896484375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999755859375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999997671694, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99993896484375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9998779296875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999923706054688, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.99993896484375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 36

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 36 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 194, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.999969482421875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.998046875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999998835847, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99993896484375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99993896484375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999961853027344, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.999969482421875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 37

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 37 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 197, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.999969482421875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.998046875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999417923, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999969482421875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99993896484375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999961853027344, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.999969482421875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 38

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 38 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 200, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.999969482421875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.998046875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999708962, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.999969482421875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99993896484375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999980926513672, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.9999847412109375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 39

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 39 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 204, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.999969482421875, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.998046875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.9998779296875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999854481, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9999847412109375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.99993896484375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999980926513672, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.9999847412109375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 40

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 40 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 211, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.9999847412109375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9990234375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99993896484375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.999999999992724, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.984375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9999847412109375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999969482421875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999990463256836, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.9999923706054688, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 41

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 41 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 217, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.9999847412109375, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.9990234375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.99993896484375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.999999999996362, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9921875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9999923706054688, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.999969482421875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999990463256836, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9921875, ((3, 1), 'east'): 0.9999923706054688, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999961853027344}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 42

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 42 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 225, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.9999923706054688, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999969482421875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.999999999998181, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9921875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9999923706054688, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9999847412109375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999995231628418, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.99609375, ((3, 1), 'east'): 0.9999961853027344, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999961853027344}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 43

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 43 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 228, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9999923706054688, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999969482421875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999990905, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.9921875, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9999923706054688, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9999847412109375, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999997615814209, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.99609375, ((3, 1), 'east'): 0.9999980926513672, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999961853027344}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 44

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 3, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 4, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 5, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
--------------------------------
EPISODE 44 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 236, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.9999923706054688, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999969482421875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999995453, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9921875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9999961853027344, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9999847412109375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999997615814209, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.99609375, ((3, 1), 'east'): 0.9999980926513672, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999980926513672}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 1.000000
################################

##################################
The policy has converged

TOTAL STEPS: 236, EPISODE STEPS: 8
##################################
Learned QValue: {((0, 1), 'east'): 0.9999923706054688, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.99951171875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.999969482421875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9999999999995453, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.99609375, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9921875, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9999961853027344, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.9999847412109375, ((0, 4), 'north'): -0.5, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.25, ((4, 1), 'north'): 0.9999997615814209, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.99609375, ((3, 1), 'east'): 0.9999980926513672, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.5, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9999980926513672}

##################################

AVERAGE RETURNS FROM START STATE: 5.63946306932


Press Enter to continuePress Enter to terminate the experiment