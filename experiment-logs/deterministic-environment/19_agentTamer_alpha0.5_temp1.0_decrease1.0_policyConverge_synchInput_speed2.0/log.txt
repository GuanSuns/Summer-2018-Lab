*************************************
*************************************
*************************************
*** Start Sub-Experiment 0 **********
*************************************
*************************************
*************************************

RUNNING 800 EPISODES

BEGINNING EPISODE: 1

Current Policy Agreement Ratio: 0.000000
Step: 0, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 3, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 4, S: (2, 3), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 6, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 7, S: (2, 4), A: west, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 8, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 10, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 11, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 12, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 13, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 14, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 15, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 16, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 17, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 18, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
--------------------------------
EPISODE 1 COMPLETE: RETURN WAS 1.50094635297

TOTAL STEPS: 19, EPISODE STEPS: 19
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.75, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.400000
################################

BEGINNING EPISODE: 2

Current Policy Agreement Ratio: 0.400000
Step: 0, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 2, S: (2, 3), A: south, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 3, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
--------------------------------
EPISODE 2 COMPLETE: RETURN WAS 0.729

TOTAL STEPS: 23, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.400000
################################

BEGINNING EPISODE: 3

Current Policy Agreement Ratio: 0.400000
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 3, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 4, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 5, S: (0, 1), A: south, S': (0, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 6, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 3 COMPLETE: RETURN WAS -5.31441

TOTAL STEPS: 30, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 4

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.533333
--------------------------------
EPISODE 4 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 32, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.533333
################################

BEGINNING EPISODE: 5

Current Policy Agreement Ratio: 0.533333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
--------------------------------
EPISODE 5 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 34, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.5, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.600000
################################

BEGINNING EPISODE: 6

Current Policy Agreement Ratio: 0.600000
Step: 0, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Uncertainty human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 3, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 6 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 40, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.75}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 7

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 7 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 42, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 8

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 8 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 44, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 9

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 9 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 50, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.984375, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 10

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (0, 3), A: west, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (0, 3), A: east, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 5, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 6, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 7, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 8, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 10 COMPLETE: RETURN WAS -4.3046721

TOTAL STEPS: 59, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.984375, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 11

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (2, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 4, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 5, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 7, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 8, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 9, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 11 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 71, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.75, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.96875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 12

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 12 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 73, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.99609375, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.75, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.5, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.875, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.96875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 13

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
--------------------------------
EPISODE 13 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 80, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.75, ((0, 2), 'west'): -0.75, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.998046875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.733333
################################

BEGINNING EPISODE: 14

Current Policy Agreement Ratio: 0.733333
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 4, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 6, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 7, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 8, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 9, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 10, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 11, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 12, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 13, S: (1, 4), A: north, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 14, S: (1, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 15, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 16, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 17, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 18, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 19, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 20, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 21, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 22, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 23, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 24, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 25, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 26, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 27, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 28, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 29, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 14 COMPLETE: RETURN WAS 0.471012869725

TOTAL STEPS: 110, EPISODE STEPS: 30
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.875, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9990234375, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.875, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9921875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 15

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 15 COMPLETE: RETURN WAS 0.4782969

TOTAL STEPS: 118, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9990234375, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9921875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 16

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 16 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 128, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99951171875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 17

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 17 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 132, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.999755859375, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 18

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (2, 1), A: west, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (3, 1), A: south, S': (3, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 18 COMPLETE: RETURN WAS -3.87420489

TOTAL STEPS: 142, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.999755859375, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 19

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 19 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 148, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9998779296875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.984375, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 20

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: south, S': (0, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 20 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 150, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9998779296875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.984375, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 21

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 4), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 21 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 155, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99993896484375, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.9375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99951171875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 22

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 3), A: west, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (0, 3), A: east, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (0, 3), A: east, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 12, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 13, S: (2, 3), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 14, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 15, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 16, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 17, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 18, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 19, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 22 COMPLETE: RETURN WAS 1.35085171767

TOTAL STEPS: 175, EPISODE STEPS: 20
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.999969482421875, ((2, 4), 'east'): 0.96875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.875, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 23

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 23 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 179, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999847412109375, ((2, 4), 'east'): 0.96875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.96875, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.875, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.5, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): 0.0625, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 24

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 3), A: south, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 24 COMPLETE: RETURN WAS 0.6561

TOTAL STEPS: 184, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999847412109375, ((2, 4), 'east'): 0.96875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.875, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.75, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.984375, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): 0.0625, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 25

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 25 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 188, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999923706054688, ((2, 4), 'east'): 0.96875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.9375, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.875, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.9921875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.75, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.984375, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.875, ((4, 3), 'east'): 0.0625, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 26

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (3, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 12, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 26 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 203, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.96875, ((4, 2), 'exit'): 0.9999961853027344, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.9375, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.99609375, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.75, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.9375, ((4, 3), 'east'): 0.0625, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 27

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 27 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 206, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.75, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.96875, ((4, 2), 'exit'): 0.9999980926513672, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.9375, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.998046875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.75, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.9375, ((4, 3), 'east'): 0.0625, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 28

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 6, S: (4, 1), A: east, S': (4, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 7, S: (4, 1), A: east, S': (4, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
--------------------------------
EPISODE 28 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 216, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.96875, ((4, 2), 'exit'): 0.9999990463256836, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.9375, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.998046875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.75, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.9375, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.9375, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.9375, ((4, 3), 'east'): 0.0625, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): -0.75, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 1.000000
################################

##################################
The policy has converged

TOTAL STEPS: 216, EPISODE STEPS: 10
##################################
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.9375, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.96875, ((4, 2), 'exit'): 0.9999990463256836, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.96875, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.875, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0.984375, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.9375, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.875, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.998046875, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): -0.75, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.9375, ((0, 4), 'north'): -0.875, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.75, ((4, 1), 'north'): 0.9375, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.9375, ((4, 3), 'east'): 0.0625, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.75, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): -0.75, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9999847412109375}

##################################

AVERAGE RETURNS FROM START STATE: 2.94382949417


Press Enter to continue*************************************
*************************************
*************************************
*** Start Sub-Experiment 1 **********
*************************************
*************************************
*************************************

RUNNING 800 EPISODES

BEGINNING EPISODE: 1

Current Policy Agreement Ratio: 0.000000
Step: 0, S: (3, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 1, S: (2, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 2, S: (2, 3), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 3, S: (2, 4), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 4, S: (2, 4), A: west, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 7, S: (3, 4), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 9, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 10, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 11, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 12, S: (3, 4), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 13, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 14, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 15, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 16, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 17, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 18, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 19, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 20, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 21, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 22, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
--------------------------------
EPISODE 1 COMPLETE: RETURN WAS 0.984770902184

TOTAL STEPS: 23, EPISODE STEPS: 23
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'south' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.333333
################################

BEGINNING EPISODE: 2

Current Policy Agreement Ratio: 0.333333
Step: 0, S: (3, 1), A: north, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 2, S: (4, 1), A: west, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 3, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 4, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 5, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
--------------------------------
EPISODE 2 COMPLETE: RETURN WAS 0.59049

TOTAL STEPS: 29, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Action 'south' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.400000
################################

BEGINNING EPISODE: 3

Current Policy Agreement Ratio: 0.400000
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 4, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 5, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 3 COMPLETE: RETURN WAS -5.9049

TOTAL STEPS: 35, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.5}

--------------------------------

Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Action 'south' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 4

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 3, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 4 COMPLETE: RETURN WAS -7.29

TOTAL STEPS: 39, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.875, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.75, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.75, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.5}

--------------------------------

Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Action 'south' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 5

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 5, S: (2, 1), A: south, S': (2, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 6, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
--------------------------------
EPISODE 5 COMPLETE: RETURN WAS -5.31441

TOTAL STEPS: 46, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.9375, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): -0.5, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.75, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.75, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): -0.5, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.5}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Action 'south' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.600000
################################

BEGINNING EPISODE: 6

Current Policy Agreement Ratio: 0.600000
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 1, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 6, S: (0, 2), A: west, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 7, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 8, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 9, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 10, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 11, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 12, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 6 COMPLETE: RETURN WAS -2.82429536481

TOTAL STEPS: 59, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): 0, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.5}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Action 'south' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 7

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (0, 3), A: east, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (0, 3), A: west, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 5, S: (1, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 6, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 7, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 8, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 9, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 11, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 12, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 13, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 14, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 15, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 16, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 7 COMPLETE: RETURN WAS 1.85302018885

TOTAL STEPS: 76, EPISODE STEPS: 17
Learned QValue: {((0, 1), 'east'): 0.96875, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): 0, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): 0, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.75}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'west' at state (2, 1) is not optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'south' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 8

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 8 COMPLETE: RETURN WAS -5.31441

TOTAL STEPS: 83, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): 0, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.5, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.75}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 9

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 3), A: west, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (2, 1), A: south, S': (2, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 9 COMPLETE: RETURN WAS -4.782969

TOTAL STEPS: 91, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): 0, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.75}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 10

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 10 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 93, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): 0, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 11

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 11 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 97, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): 0, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.75, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): 0, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0.5, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 12

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 9, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 10, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 12 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 110, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.75, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): 0, ((0, 3), 'west'): -0.75, ((2, 4), 'south'): 0, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.25, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.96875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 13

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 3), A: west, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (3, 4), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 6, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 8, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 10, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 11, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 12, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 13, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 14, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 15, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 16, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 17, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 18, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 19, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 20, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 13 COMPLETE: RETURN WAS 1.21576654591

TOTAL STEPS: 131, EPISODE STEPS: 21
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.984375, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.9375, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.875, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 14

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (0, 4), A: north, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 4, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 5, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 14 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 142, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.40625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9921875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 15

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 15 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 151, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): 0, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99609375, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.9375, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.8515625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 16

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: west, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 16 COMPLETE: RETURN WAS 0.59049

TOTAL STEPS: 157, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99609375, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.9375, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.8515625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 17

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 17 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 160, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.5, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.5, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.998046875, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.9375, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.8515625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.75, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 18

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 18 COMPLETE: RETURN WAS -5.9049

TOTAL STEPS: 166, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.998046875, ((2, 4), 'east'): 0.984375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.9375, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.8515625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 19

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 19 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 171, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.99609375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9990234375, ((2, 4), 'east'): 0.9921875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.9375, ((3, 4), 'east'): 0.984375, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): -0.5, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.96875, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.8515625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 20

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 20 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 181, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.998046875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99951171875, ((2, 4), 'east'): 0.99609375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.99609375, ((1, 4), 'north'): -0.5, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.25, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.984375, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.8515625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.9375, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.99951171875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 21

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 13, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 21 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 197, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.999755859375, ((2, 4), 'east'): 0.998046875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.998046875, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.25, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.5, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.92578125, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 22

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 1), A: north, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (3, 1), A: south, S': (3, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 22 COMPLETE: RETURN WAS -7.29

TOTAL STEPS: 201, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.999755859375, ((2, 4), 'east'): 0.998046875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.998046875, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0.25, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9921875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9375, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.92578125, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.999755859375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 23

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 23 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 208, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.99951171875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9998779296875, ((2, 4), 'east'): 0.9990234375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.998046875, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0.25, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.984375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.99609375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.96875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.92578125, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.9998779296875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 24

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 24 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 222, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.999755859375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.99993896484375, ((2, 4), 'east'): 0.99951171875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.5, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.9375, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.998046875, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.5, ((3, 3), 'west'): -0.5, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0.25, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9990234375, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.9375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.99609375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.984375, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.92578125, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.96875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.984375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.99993896484375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 25

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (3, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (2, 3), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (2, 4), A: north, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 12, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 13, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 14, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 15, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 16, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 17, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 18, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 19, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 25 COMPLETE: RETURN WAS 1.35085171767

TOTAL STEPS: 242, EPISODE STEPS: 20
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.99993896484375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.999969482421875, ((2, 4), 'east'): 0.9998779296875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.9375, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9990234375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.96875, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.998046875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.984375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9921875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.999969482421875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 26

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 26 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 245, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.999969482421875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999847412109375, ((2, 4), 'east'): 0.9998779296875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.9375, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9990234375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.96875, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.998046875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.984375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9921875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.9999847412109375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 27

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 27 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 248, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999847412109375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.9375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999923706054688, ((2, 4), 'east'): 0.9998779296875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.9375, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9990234375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.96875, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.998046875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.984375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9921875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 28

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 28 COMPLETE: RETURN WAS -7.29

TOTAL STEPS: 252, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999847412109375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.96875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999923706054688, ((2, 4), 'east'): 0.9998779296875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.9375, ((4, 1), 'south'): -0.875, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9990234375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.99951171875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.96875, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.875, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.998046875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.984375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9921875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.9999923706054688}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 29

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 29 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 257, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999847412109375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.75, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.96875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999961853027344, ((2, 4), 'east'): 0.9998779296875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.9375, ((4, 1), 'south'): -0.875, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9990234375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.875, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.998046875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.984375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9921875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.9999961853027344}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 30

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 30 COMPLETE: RETURN WAS 0.6561

TOTAL STEPS: 262, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999847412109375, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999961853027344, ((2, 4), 'east'): 0.9998779296875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.9375, ((4, 1), 'south'): -0.875, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9990234375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.875, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.998046875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.984375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9921875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999961853027344}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 31

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 31 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 266, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999923706054688, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.875, ((4, 2), 'exit'): 0.9999980926513672, ((2, 4), 'east'): 0.9998779296875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.96875, ((4, 1), 'south'): -0.875, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9990234375, ((1, 4), 'north'): -0.75, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.875, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.998046875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.984375, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.9921875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999980926513672}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 32

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 32 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 281, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999980926513672, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999990463256836, ((2, 4), 'east'): 0.99993896484375, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.75, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.96875, ((4, 1), 'south'): -0.875, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.999755859375, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.875, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.875, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9990234375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9921875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.99609375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999990463256836}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 33

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 33 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 286, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999990463256836, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999995231628418, ((2, 4), 'east'): 0.999969482421875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.75, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.96875, ((4, 1), 'south'): -0.875, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9998779296875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.875, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.875, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9990234375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9921875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.99609375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999995231628418}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 34

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 34 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 288, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9999990463256836, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): -0.875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.984375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999995231628418, ((2, 4), 'east'): 0.999969482421875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.75, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.96875, ((4, 1), 'south'): -0.9375, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9998779296875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.875, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.9375, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.984375, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9990234375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.75, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9921875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.99609375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999995231628418}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 35

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (3, 1), A: north, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (2, 1), A: west, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 12, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 13, S: (3, 1), A: west, S': (2, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 14, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 15, S: (3, 1), A: north, S': (3, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 16, S: (3, 1), A: south, S': (3, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 17, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 35 COMPLETE: RETURN WAS -1.66771816997

TOTAL STEPS: 306, EPISODE STEPS: 18
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9999990463256836, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): -0.9921875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.99951171875, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999995231628418, ((2, 4), 'east'): 0.999969482421875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.75, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.96875, ((4, 1), 'south'): -0.9375, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9998779296875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.875, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.9375, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.99609375, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9990234375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0, ((3, 1), 'north'): -0.9375, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9921875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.9375, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.99609375, ((1, 1), 'north'): -0.875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999995231628418}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'east' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 36

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
--------------------------------
EPISODE 36 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 312, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.998046875, ((4, 4), 'south'): 0.9999990463256836, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): -0.9921875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.999755859375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999997615814209, ((2, 4), 'east'): 0.999969482421875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.75, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.96875, ((4, 1), 'south'): -0.9375, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9998779296875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.875, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.9375, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.998046875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9990234375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.9375, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9921875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.96875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.99609375, ((1, 1), 'north'): -0.875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999995231628418}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 1.000000
################################

##################################
The policy has converged

TOTAL STEPS: 312, EPISODE STEPS: 6
##################################
Learned QValue: {((0, 1), 'east'): 0.998046875, ((4, 4), 'south'): 0.9999990463256836, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): -0.875, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): -0.9921875, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0.999755859375, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.9375, ((4, 2), 'exit'): 0.9999997615814209, ((2, 4), 'east'): 0.999969482421875, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.75, ((3, 3), 'south'): -0.9375, ((3, 3), 'north'): -0.875, ((3, 4), 'north'): -0.875, ((4, 4), 'north'): -0.96875, ((4, 1), 'south'): -0.9375, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.75, ((0, 3), 'west'): -0.875, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.96875, ((3, 4), 'east'): 0.9998779296875, ((1, 4), 'north'): -0.875, ((0, 2), 'north'): -0.75, ((2, 3), 'north'): -0.75, ((3, 3), 'west'): -0.75, ((4, 4), 'west'): -0.875, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): -0.375, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.75, ((3, 3), 'east'): 0.9998779296875, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.984375, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.9375, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.998046875, ((0, 4), 'north'): -0.75, ((1, 4), 'east'): 0.9990234375, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): -0.9375, ((3, 4), 'south'): 0.9921875, ((1, 4), 'south'): 0.25, ((4, 3), 'east'): -0.962890625, ((4, 1), 'west'): -0.5, ((0, 4), 'west'): -0.96875, ((2, 4), 'west'): -0.5, ((4, 3), 'north'): -0.9921875, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0.96875, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.99609375, ((1, 1), 'north'): -0.875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9999995231628418}

##################################

AVERAGE RETURNS FROM START STATE: 1.24496138807


Press Enter to continue*************************************
*************************************
*************************************
*** Start Sub-Experiment 2 **********
*************************************
*************************************
*************************************

RUNNING 800 EPISODES

BEGINNING EPISODE: 1

Current Policy Agreement Ratio: 0.000000
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.000000
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 3, S: (0, 1), A: south, S': (0, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 4, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
--------------------------------
EPISODE 1 COMPLETE: RETURN WAS -6.561

TOTAL STEPS: 5, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Action 'north' at state (3, 3) is not optimal
Action 'north' at state (3, 4) is not optimal
Action 'west' at state (4, 1) is not optimal
Action 'north' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.066667
################################

BEGINNING EPISODE: 2

Current Policy Agreement Ratio: 0.066667
Step: 0, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
Step: 1, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.066667
--------------------------------
EPISODE 2 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 7, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0, ((2, 4), 'east'): 0, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): 0, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): 0, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): 0, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Action 'north' at state (3, 1) is not optimal
Action 'north' at state (3, 3) is not optimal
Action 'north' at state (3, 4) is not optimal
Action 'west' at state (4, 1) is not optimal
Action 'north' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.066667
################################

BEGINNING EPISODE: 3

Current Policy Agreement Ratio: 0.066667
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 1, S: (2, 3), A: west, S': (2, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.133333
Step: 2, S: (2, 3), A: east, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 3, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.200000
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.266667
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 6, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 7, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 8, S: (3, 3), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 9, S: (3, 4), A: west, S': (2, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 11, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 12, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 13, S: (3, 3), A: south, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.333333
Step: 14, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.400000
Step: 15, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 16, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 17, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 18, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
Step: 19, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.466667
--------------------------------
EPISODE 3 COMPLETE: RETURN WAS 1.35085171767

TOTAL STEPS: 27, EPISODE STEPS: 20
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.75, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0.5, ((2, 4), 'east'): 0.5, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): 0, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.5, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.5, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.5, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): 0, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.5}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.466667
################################

BEGINNING EPISODE: 4

Current Policy Agreement Ratio: 0.466667
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.533333
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 4, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.600000
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 7, S: (3, 4), A: north, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 9, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 12, S: (4, 3), A: west, S': (3, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 13, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 4 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 43, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.5, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.5, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.75}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 5

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (0, 1), A: south, S': (0, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 5 COMPLETE: RETURN WAS -7.29

TOTAL STEPS: 47, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.875, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.75, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0.875, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.75}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Action 'south' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 6

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
--------------------------------
EPISODE 6 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 52, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.5, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.5, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): 0, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): 0, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.5, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Action 'north' at state (0, 1) is not optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Action 'south' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.666667
################################

BEGINNING EPISODE: 7

Current Policy Agreement Ratio: 0.666667
Step: 0, S: (0, 3), A: west, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 1, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 2, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 4, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 5, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 6, S: (0, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.666667
Step: 7, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 8, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 9, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 10, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 11, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 12, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.733333
Step: 13, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 14, S: (2, 1), A: south, S': (2, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 15, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 7 COMPLETE: RETURN WAS -2.05891132095

TOTAL STEPS: 68, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.875, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): 0, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.75, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 8

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (3, 1), A: south, S': (3, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 8 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 70, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.5, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.9375, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.875, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.75, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.75, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.5, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 9

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 4, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 5, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 9 COMPLETE: RETURN WAS -5.9049

TOTAL STEPS: 76, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.9375, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.875, ((2, 4), 'east'): 0.75, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.875, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.5, ((3, 4), 'east'): 0.9375, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.5, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.5, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.5, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.75, ((0, 4), 'east'): 0.75, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 10

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 3, S: (1, 4), A: south, S': (1, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 7, S: (4, 4), A: east, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 10 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 87, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 0.75, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.96875, ((0, 2), 'west'): 0, ((2, 1), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.875, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.5, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.5, ((1, 1), 'west'): -0.5, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.75, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.75, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 11

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (2, 1), A: west, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 6, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 7, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 8, S: (0, 2), A: east, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 9, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 10, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 11, S: (1, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 12, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 13, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
Step: 14, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.800000
--------------------------------
EPISODE 11 COMPLETE: RETURN WAS -2.2876792455

TOTAL STEPS: 102, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.9375, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.75, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.75, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.875, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.800000
################################

BEGINNING EPISODE: 12

Current Policy Agreement Ratio: 0.800000
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.866667
--------------------------------
EPISODE 12 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 104, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.75, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.75, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): 0, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.875, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.866667
################################

BEGINNING EPISODE: 13

Current Policy Agreement Ratio: 0.866667
Step: 0, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 13 COMPLETE: RETURN WAS 0.9

TOTAL STEPS: 106, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.984375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.75, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -0.75, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.75, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.75, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): 0, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.875, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.875, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 14

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 3), A: east, S': (0, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (0, 3), A: south, S': (0, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (0, 1), A: south, S': (0, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 14 COMPLETE: RETURN WAS -5.31441

TOTAL STEPS: 113, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.96875, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.96875, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.5, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.96875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.75, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.75, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.5, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.875, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.9375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 15

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 15 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 119, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.984375, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.9921875, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.984375, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.984375, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.75, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.75, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.875, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.875, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.96875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 16

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (1, 1), A: south, S': (1, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 16 COMPLETE: RETURN WAS -5.9049

TOTAL STEPS: 125, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.984375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.5, ((4, 2), 'exit'): 0.984375, ((2, 4), 'east'): 0.875, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.984375, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.5, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.75, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.75, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.5, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.5, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.9375, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.96875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 17

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 4), A: west, S': (3, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 8, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 17 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 137, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.5, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.5, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 18

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: south, S': (4, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 18 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 139, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.9921875, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.5, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.5, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.5, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 19

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: south, S': (2, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 19 COMPLETE: RETURN WAS -7.29

TOTAL STEPS: 143, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.5, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.5, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 20

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: south, S': (3, 0), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 20 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 145, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.5, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.75, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.5, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 21

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 21 COMPLETE: RETURN WAS 0.81

TOTAL STEPS: 148, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9921875, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9921875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.984375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 22

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 22 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 151, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.99609375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99609375, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.75, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.75, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.9921875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 23

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 4), A: south, S': (3, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: east, S': (4, 3), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 23 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 156, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.99609375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.5, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.998046875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.75, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.75, ((2, 1), 'north'): -0.75, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 24

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (2, 1), A: west, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (1, 1), A: north, S': (1, 1), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (2, 1), A: north, S': (2, 2), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 7, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 24 COMPLETE: RETURN WAS 0.4782969

TOTAL STEPS: 164, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.99609375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.998046875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.9375, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.96875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.99609375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 25

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 25 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 167, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.99609375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9990234375, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.5, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.96875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 26

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 26 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 169, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.99609375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.99951171875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.75, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.75, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.96875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.998046875}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 27

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 2, S: (4, 3), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 3, S: (4, 4), A: north, S': (4, 4), R: 0.0

Receive Negative (-1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 0.933333
--------------------------------
EPISODE 27 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 176, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.999755859375, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.75, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.9375, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.96875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.933333
################################

BEGINNING EPISODE: 28

Current Policy Agreement Ratio: 0.933333
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Receive Positive (+1) human signal

Current Policy Agreement Ratio: 1.000000
--------------------------------
EPISODE 28 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 179, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9998779296875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.9375, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.96875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9990234375}

--------------------------------

Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 3) is optimal
Policy at state (2, 4) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 1.000000
################################

##################################
The policy has converged

TOTAL STEPS: 179, EPISODE STEPS: 3
##################################
Learned QValue: {((0, 1), 'east'): 0.99609375, ((4, 4), 'south'): 0.9990234375, ((0, 2), 'south'): 0.99609375, ((0, 2), 'west'): 0, ((2, 1), 'west'): -0.75, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -0.75, ((2, 1), 'east'): 0, ((2, 3), 'west'): -0.5, ((0, 3), 'north'): 0.75, ((4, 2), 'exit'): 0.9998779296875, ((2, 4), 'east'): 0.9375, ((0, 1), 'south'): -0.875, ((2, 3), 'east'): 0.5, ((0, 2), 'east'): -0.9375, ((3, 3), 'south'): -0.75, ((3, 3), 'north'): -0.75, ((3, 4), 'north'): -0.5, ((4, 4), 'north'): -0.875, ((4, 1), 'south'): -0.75, ((2, 1), 'south'): -0.75, ((3, 4), 'west'): -0.5, ((0, 3), 'west'): -0.5, ((2, 4), 'south'): 0.5, ((4, 4), 'east'): -0.75, ((3, 4), 'east'): 0.9921875, ((1, 4), 'north'): 0, ((0, 2), 'north'): -0.5, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): -0.75, ((0, 3), 'east'): -0.5, ((1, 0), 'exit'): -0.875, ((3, 1), 'south'): -0.75, ((2, 4), 'north'): 0, ((1, 1), 'south'): -0.875, ((1, 1), 'west'): -0.96875, ((3, 3), 'east'): 0.96875, ((0, 0), 'exit'): -0.875, ((2, 3), 'south'): 0, ((3, 0), 'exit'): -0.75, ((2, 2), 'exit'): -0.875, ((4, 3), 'west'): -0.75, ((0, 1), 'north'): -0.9375, ((4, 0), 'exit'): -0.75, ((1, 4), 'west'): -0.75, ((1, 1), 'east'): 0.96875, ((0, 4), 'north'): 0, ((1, 4), 'east'): 0.875, ((0, 4), 'south'): -0.5, ((4, 1), 'north'): 0.875, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0.875, ((1, 4), 'south'): -0.75, ((4, 3), 'east'): -0.875, ((4, 1), 'west'): 0, ((0, 4), 'west'): -0.5, ((2, 4), 'west'): 0, ((4, 3), 'north'): -0.9375, ((0, 3), 'south'): 0.9375, ((3, 1), 'east'): 0.5, ((4, 1), 'east'): 0, ((0, 1), 'west'): -0.9375, ((0, 4), 'east'): 0.96875, ((1, 1), 'north'): -0.96875, ((2, 1), 'north'): -0.875, ((4, 3), 'south'): 0.9990234375}

##################################

AVERAGE RETURNS FROM START STATE: 0.00901641907417


Press Enter to continuePress Enter to terminate the experiment