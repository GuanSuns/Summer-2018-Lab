
RUNNING 1000 EPISODES

BEGINNING EPISODE: 1

Current Policy Convergence Ratio: 0.318182
Step: 0, S: (2, 3), A: south, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 1, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.318182
--------------------------------
EPISODE 1 COMPLETE: RETURN WAS 0.9

TOTAL STEPS: 2, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 0, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0.0, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0, ((4, 3), 'west'): 0, ((4, 1), 'north'): 0, ((0, 1), 'north'): 0, ((2, 4), 'west'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0, ((3, 4), 'west'): 0, ((3, 1), 'north'): 0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0, ((0, 4), 'west'): 0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'north' at state (3, 3) is not optimal
Action 'north' at state (3, 4) is not optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Action 'north' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.318182
################################

BEGINNING EPISODE: 2

Current Policy Convergence Ratio: 0.318182
Step: 0, S: (4, 1), A: west, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 1, S: (3, 1), A: north, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.318182
--------------------------------
EPISODE 2 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 7, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0, ((0, 2), 'west'): 0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0, ((4, 2), 'exit'): 5.0, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): 0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0, ((1, 1), 'west'): 0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0.0, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0, ((4, 3), 'west'): 0, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0, ((2, 4), 'west'): 0, ((1, 4), 'west'): 0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0, ((3, 4), 'west'): 0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0, ((0, 4), 'east'): 0, ((1, 1), 'north'): 0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'north' at state (3, 3) is not optimal
Action 'north' at state (3, 4) is not optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Action 'north' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.318182
################################

BEGINNING EPISODE: 3

Current Policy Convergence Ratio: 0.318182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 4, S: (0, 4), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 5, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 6, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 7, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 8, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 9, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 10, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 11, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 12, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 13, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 14, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 15, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 16, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 17, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 18, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 19, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 20, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 21, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 22, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 23, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 24, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 25, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 26, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 27, S: (1, 1), A: south, S': (1, 0), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 28, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.318182
--------------------------------
EPISODE 3 COMPLETE: RETURN WAS -0.523347633027

TOTAL STEPS: 36, EPISODE STEPS: 29
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 5.0, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.5, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -5.0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0.0, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0.0, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0, ((4, 3), 'west'): 0, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'north' at state (3, 3) is not optimal
Action 'north' at state (3, 4) is not optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Action 'north' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.318182
################################

BEGINNING EPISODE: 4

Current Policy Convergence Ratio: 0.318182
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 1, S: (2, 3), A: south, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 2, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.318182
--------------------------------
EPISODE 4 COMPLETE: RETURN WAS 0.81

TOTAL STEPS: 39, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 5.0, ((2, 4), 'east'): 0, ((0, 1), 'south'): 0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0, ((3, 3), 'north'): 0, ((3, 4), 'north'): 0, ((4, 4), 'north'): 0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0, ((3, 4), 'east'): 0, ((1, 4), 'north'): 0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0, ((3, 3), 'west'): 0, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0, ((1, 0), 'exit'): -5.0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0, ((1, 1), 'south'): 0.0, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): 0, ((2, 3), 'south'): 0.225, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0, ((4, 3), 'west'): 0, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0, ((1, 4), 'south'): 0, ((4, 3), 'east'): 0, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'north' at state (3, 3) is not optimal
Action 'north' at state (3, 4) is not optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Action 'north' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.318182
################################

BEGINNING EPISODE: 5

Current Policy Convergence Ratio: 0.318182
Step: 0, S: (4, 4), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 2, S: (4, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 3, S: (4, 3), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 4, S: (4, 4), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 5, S: (4, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 7, S: (4, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 8, S: (4, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 9, S: (4, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 10, S: (4, 3), A: west, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 11, S: (3, 3), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 12, S: (3, 4), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 13, S: (3, 4), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 14, S: (3, 3), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 15, S: (3, 3), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 16, S: (3, 4), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 17, S: (3, 3), A: west, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 18, S: (2, 3), A: west, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 19, S: (2, 3), A: west, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 20, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 21, S: (2, 4), A: west, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 22, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 23, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 24, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 25, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 26, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 27, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 28, S: (3, 4), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.318182
Step: 29, S: (3, 4), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 30, S: (3, 3), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 31, S: (3, 3), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 32, S: (3, 3), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 33, S: (3, 3), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 34, S: (3, 4), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 35, S: (3, 4), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 36, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 37, S: (4, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 38, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 39, S: (4, 3), A: west, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 40, S: (3, 3), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 41, S: (3, 4), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 42, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 43, S: (2, 4), A: west, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 44, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 45, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 46, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 47, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 48, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 49, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 50, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 51, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 52, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 53, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 54, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 55, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 56, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 57, S: (0, 1), A: south, S': (0, 0), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 58, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.363636
--------------------------------
EPISODE 5 COMPLETE: RETURN WAS -0.0221853123446

TOTAL STEPS: 98, EPISODE STEPS: 59
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0.0, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.151875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 5.0, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): 0.0, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.07973437500000001, ((3, 3), 'north'): 0.030754687500000006, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.75, ((0, 3), 'west'): 0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.0, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.10125, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -5.0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): 0.0, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): -5.0, ((2, 3), 'south'): 0.225, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.045562500000000006, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.045562500000000006, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.0, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.0, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Action 'west' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.363636
################################

BEGINNING EPISODE: 6

Current Policy Convergence Ratio: 0.363636
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 1, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 5, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 6, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 7, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 8, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 9, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 10, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 11, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 12, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 13, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 14, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 15, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 16, S: (0, 1), A: south, S': (0, 0), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 17, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.363636
--------------------------------
EPISODE 6 COMPLETE: RETURN WAS -1.66771816997

TOTAL STEPS: 116, EPISODE STEPS: 18
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0.0, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.151875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 5.0, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.07973437500000001, ((3, 3), 'north'): 0.030754687500000006, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.0, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.75, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.0, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.10125, ((4, 4), 'west'): 0, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -5.0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): 0.0, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.225, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.045562500000000006, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.045562500000000006, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.0, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.0, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Action 'west' at state (4, 3) is not optimal
Action 'north' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.363636
################################

BEGINNING EPISODE: 7

Current Policy Convergence Ratio: 0.363636
Step: 0, S: (4, 4), A: west, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 1, S: (3, 4), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 2, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 3, S: (4, 3), A: west, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 4, S: (3, 3), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 5, S: (3, 4), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 6, S: (3, 3), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 7, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 8, S: (4, 3), A: west, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 9, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 10, S: (4, 3), A: west, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 11, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 12, S: (4, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 13, S: (4, 3), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 14, S: (4, 4), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 15, S: (4, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 16, S: (4, 4), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 17, S: (4, 4), A: west, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 18, S: (3, 4), A: south, S': (3, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 19, S: (3, 3), A: west, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 20, S: (2, 3), A: west, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 21, S: (2, 3), A: south, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 22, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.363636
--------------------------------
EPISODE 7 COMPLETE: RETURN WAS 0.0984770902184

TOTAL STEPS: 139, EPISODE STEPS: 23
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0.0, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 5.0, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.013839609375000004, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.009226406250000003, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -5.0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): 0.0, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.03844335937500001, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.009226406250000003, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Action 'west' at state (4, 3) is not optimal
Action 'west' at state (4, 4) is not optimal
################################
Current Policy Convergence Ratio: 0.363636
################################

BEGINNING EPISODE: 8

Current Policy Convergence Ratio: 0.363636
Step: 0, S: (4, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 1, S: (4, 3), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 2, S: (4, 4), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 4, S: (4, 3), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 5, S: (4, 4), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 6, S: (4, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 7, S: (4, 4), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.363636
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.409091
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.454545
--------------------------------
EPISODE 8 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 150, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0.057665039062500006, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): 0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 7.5, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -5.0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): 0.0, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 2.25}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.454545
################################

BEGINNING EPISODE: 9

Current Policy Convergence Ratio: 0.454545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 4, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 6, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 7, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 8, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 9, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 10, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 11, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 12, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 13, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 14, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 15, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 16, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 17, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 18, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 19, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 20, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 21, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 22, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 23, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 24, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 25, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 26, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 27, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 28, S: (2, 1), A: south, S': (2, 0), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 29, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.454545
--------------------------------
EPISODE 9 COMPLETE: RETURN WAS -0.471012869725

TOTAL STEPS: 180, EPISODE STEPS: 30
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0.057665039062500006, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 7.5, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -5.0, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): 0.0, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 2.25}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.454545
################################

BEGINNING EPISODE: 10

Current Policy Convergence Ratio: 0.454545
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 6, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 7, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 8, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 9, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 10, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 11, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 12, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 13, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 14, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 15, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 16, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 17, S: (1, 1), A: south, S': (1, 0), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 18, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.454545
--------------------------------
EPISODE 10 COMPLETE: RETURN WAS -1.50094635297

TOTAL STEPS: 199, EPISODE STEPS: 19
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0.057665039062500006, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 7.5, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 0.0, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 2.25}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Action 'west' at state (4, 1) is not optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.454545
################################

BEGINNING EPISODE: 11

Current Policy Convergence Ratio: 0.454545
Step: 0, S: (4, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.454545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.500000
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.500000
--------------------------------
EPISODE 11 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 202, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 0.057665039062500006, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 8.75, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 3.375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 2.25}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.500000
################################

BEGINNING EPISODE: 12

Current Policy Convergence Ratio: 0.500000
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.500000
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.500000
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.500000
--------------------------------
EPISODE 12 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 205, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 1.04133251953125, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.375, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 3.375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 5.0625}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.500000
################################

BEGINNING EPISODE: 13

Current Policy Convergence Ratio: 0.500000
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.500000
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.500000
--------------------------------
EPISODE 13 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 207, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 1.04133251953125, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.6875, ((2, 4), 'east'): 0.0, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.035880468750000005, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.0, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 3.375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 6.75}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Action 'north' at state (2, 4) is not optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.500000
################################

BEGINNING EPISODE: 14

Current Policy Convergence Ratio: 0.500000
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.500000
Step: 1, S: (2, 4), A: west, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.500000
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.500000
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 4, S: (3, 4), A: north, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.545455
--------------------------------
EPISODE 14 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 216, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 3.558166259765625, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.84375, ((2, 4), 'east'): 0.03844335937500001, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.4685996337890625, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 3.375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 7.734375}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.545455
################################

BEGINNING EPISODE: 15

Current Policy Convergence Ratio: 0.545455
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.545455
--------------------------------
EPISODE 15 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 219, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 5.259551879882812, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.921875, ((2, 4), 'east'): 0.03844335937500001, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.4685996337890625, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 3.375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.296875}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.545455
################################

BEGINNING EPISODE: 16

Current Policy Convergence Ratio: 0.545455
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.545455
--------------------------------
EPISODE 16 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 221, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 5.259551879882812, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.9609375, ((2, 4), 'east'): 0.03844335937500001, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 0.4685996337890625, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.0, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.0, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.296875}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Action 'north' at state (1, 4) is not optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.545455
################################

BEGINNING EPISODE: 17

Current Policy Convergence Ratio: 0.545455
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 2, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 4, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 5, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 6, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 7, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 8, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 9, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 10, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 11, S: (0, 4), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 12, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 13, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 14, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 15, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 16, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 17, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 18, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 19, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 20, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 21, S: (0, 4), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 22, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 23, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 24, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 25, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 26, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 27, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.545455
Step: 28, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 29, S: (2, 4), A: west, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 30, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 31, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 32, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 33, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 34, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 35, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.590909
--------------------------------
EPISODE 17 COMPLETE: RETURN WAS 0.250315550499

TOTAL STEPS: 257, EPISODE STEPS: 36
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 6.363369689941406, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.98046875, ((2, 4), 'east'): 0.23009151489257812, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 2.601098162841797, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): 0, ((1, 4), 'east'): 0.025949267578125004, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.007784780273437502, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.630859375}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.590909
################################

BEGINNING EPISODE: 18

Current Policy Convergence Ratio: 0.590909
Step: 0, S: (3, 1), A: south, S': (3, 0), R: 0.0

Current Policy Convergence Ratio: 0.590909
Step: 1, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.590909
--------------------------------
EPISODE 18 COMPLETE: RETURN WAS -9.0

TOTAL STEPS: 259, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 6.363369689941406, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.98046875, ((2, 4), 'east'): 0.23009151489257812, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 2.601098162841797, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0.0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 0.06663515625000001, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -5.0, ((1, 4), 'east'): 0.025949267578125004, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.007784780273437502, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.630859375}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Action 'west' at state (3, 3) is not optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.590909
################################

BEGINNING EPISODE: 19

Current Policy Convergence Ratio: 0.590909
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.636364
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.636364
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.636364
--------------------------------
EPISODE 19 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 262, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 6.363369689941406, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.990234375, ((2, 4), 'east'): 0.23009151489257812, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 2.601098162841797, ((1, 4), 'north'): 0.0, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0.0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -5.0, ((1, 4), 'east'): 0.025949267578125004, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.007784780273437502, ((1, 4), 'west'): 0.0, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.0, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.0, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.806640625}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Action 'north' at state (0, 4) is not optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.636364
################################

BEGINNING EPISODE: 20

Current Policy Convergence Ratio: 0.636364
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 3, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 5, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 6, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 7, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.681818
--------------------------------
EPISODE 20 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 276, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 7.144673126220702, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.0, ((4, 2), 'exit'): 9.9951171875, ((2, 4), 'east'): 1.2855399307250976, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 4.1640654418945315, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0.0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -5.0, ((1, 4), 'east'): 0.11651581549072265, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.007784780273437502, ((1, 4), 'west'): 0.005254726684570313, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.021894694519042975, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.017515755615234378, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.89892578125}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Action 'west' at state (0, 3) is not optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.681818
################################

BEGINNING EPISODE: 21

Current Policy Convergence Ratio: 0.681818
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 6, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 7, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 8, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 9, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 10, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 11, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.681818
Step: 12, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 13, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 14, S: (1, 4), A: south, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 15, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 16, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 17, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 18, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 19, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 20, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 21 COMPLETE: RETURN WAS 1.21576654591

TOTAL STEPS: 297, EPISODE STEPS: 21
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 7.576853164672851, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.00788209002685547, ((4, 2), 'exit'): 9.99755859375, ((2, 4), 'east'): 2.5165994142150883, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 5.297135627746582, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 0.0, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0.0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -5.0, ((1, 4), 'east'): 0.6367508765716553, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.007784780273437502, ((1, 4), 'west'): 0.005254726684570313, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.06118999477844238, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.947265625}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'south' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 22

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (2, 4), A: west, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 22 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 305, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 7.814696113586426, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.00788209002685547, ((4, 2), 'exit'): 9.998779296875, ((2, 4), 'east'): 3.642010739593506, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 6.058151737976074, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): 0.0, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -5.0, ((1, 4), 'east'): 1.4508451746826174, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 0.005254726684570313, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.06118999477844238, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.9725341796875}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 23

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (3, 1), A: south, S': (3, 0), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (3, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 23 COMPLETE: RETURN WAS -5.9049

TOTAL STEPS: 311, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 7.814696113586426, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -5.0, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.00788209002685547, ((4, 2), 'exit'): 9.998779296875, ((2, 4), 'east'): 3.642010739593506, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): 0.0, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 6.058151737976074, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 1.4508451746826174, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 0.005254726684570313, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.06118999477844238, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.9725341796875}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 24

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (2, 1), A: south, S': (2, 0), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (2, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 24 COMPLETE: RETURN WAS -8.1

TOTAL STEPS: 314, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 7.814696113586426, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.00788209002685547, ((4, 2), 'exit'): 9.998779296875, ((2, 4), 'east'): 3.642010739593506, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.0, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 6.058151737976074, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.0, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.0, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 1.4508451746826174, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 0.005254726684570313, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.0, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.0, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.0, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 0.06118999477844238, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.9725341796875}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 25

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 6, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 7, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 8, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 9, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 10, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 11, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 12, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 13, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 14, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 15, S: (0, 4), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 16, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 17, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 18, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 19, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 20, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 21, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 22, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 23, S: (0, 3), A: east, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 24, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 25, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 26, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 27, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 28, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 29, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 30, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 31, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 32, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 33, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 34, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 35, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 36, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 25 COMPLETE: RETURN WAS 0.225283995449

TOTAL STEPS: 351, EPISODE STEPS: 37
Learned QValue: {((0, 1), 'east'): 0.0, ((4, 4), 'south'): 7.944988437652588, ((0, 2), 'south'): 0.0, ((0, 2), 'west'): 0.0023941848456573494, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.323302168030243, ((4, 2), 'exit'): 9.9993896484375, ((2, 4), 'east'): 4.547173651885986, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.0023941848456573494, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.014164444198677063, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 6.545689120101929, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.016824649582740783, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -7.5, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -2.25, ((1, 1), 'west'): 0.0, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 2.3643274201583866, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 0.6089007598197557, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0031922464608764656, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 1.1501893244098664, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.9857177734375}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'north' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 26

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 6, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 7, S: (1, 1), A: south, S': (1, 0), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 8, S: (1, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 26 COMPLETE: RETURN WAS -4.3046721

TOTAL STEPS: 360, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.00032321495416374216, ((4, 4), 'south'): 7.944988437652588, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.323302168030243, ((4, 2), 'exit'): 9.9993896484375, ((2, 4), 'east'): 4.547173651885986, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.008768184735062027, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.014164444198677063, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 6.545689120101929, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.016824649582740783, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.0007182554536972048, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 2.3643274201583866, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 0.6089007598197557, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0031922464608764656, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 1.1501893244098664, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.9857177734375}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 27

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 27 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 362, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.00032321495416374216, ((4, 4), 'south'): 7.944988437652588, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.323302168030243, ((4, 2), 'exit'): 9.99969482421875, ((2, 4), 'east'): 4.547173651885986, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.008768184735062027, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.014164444198677063, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 6.545689120101929, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.016824649582740783, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.0007182554536972048, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 2.3643274201583866, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 0.6089007598197557, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.0031922464608764656, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 1.1501893244098664, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.992584228515625}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 28

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (0, 3), A: west, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 8, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 9, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 10, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 28 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 378, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 0.00032321495416374216, ((4, 4), 'south'): 8.019157121658326, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.6792362799995614, ((4, 2), 'exit'): 9.999847412109375, ((2, 4), 'east'): 5.219146929988861, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.011955184679764367, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 6.848089356994629, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.2224351258160992, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.0007182554536972048, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 3.2283918534278873, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 1.8834683397093774, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.99615478515625}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 29

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 29 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 384, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.00032321495416374216, ((4, 4), 'south'): 8.057848214149477, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.6792362799995614, ((4, 2), 'exit'): 9.999923706054688, ((2, 4), 'east'): 5.691213675642014, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.011955184679764367, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.032665383243561, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.2224351258160992, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.0007182554536972048, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 3.9628120452089313, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 1.8834683397093774, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0, ((4, 3), 'south'): 8.998008728027344}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 30

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 6, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 30 COMPLETE: RETURN WAS 0.531441

TOTAL STEPS: 391, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.000888841123950291, ((4, 4), 'south'): 8.057848214149477, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.6792362799995614, ((4, 2), 'exit'): 9.999923706054688, ((2, 4), 'east'): 5.691213675642014, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.011955184679764367, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.032665383243561, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.2224351258160992, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.0012569470439701084, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 3.9628120452089313, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 1.8834683397093774, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0.39375, ((4, 3), 'south'): 8.998008728027344}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 31

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.727273
--------------------------------
EPISODE 31 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 398, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.000888841123950291, ((4, 4), 'south'): 8.078028034687044, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.6792362799995614, ((4, 2), 'exit'): 9.999961853027344, ((2, 4), 'east'): 6.010306260280609, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.011955184679764367, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.142364387989045, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.2224351258160992, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.0012569470439701084, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 4.542452176643372, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.0, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.0, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 2.7249995901987076, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0.39375, ((4, 3), 'south'): 8.998970031738281}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'north' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.727273
################################

BEGINNING EPISODE: 32

Current Policy Convergence Ratio: 0.727273
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.727273
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (3, 1), A: west, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 8, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 9, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 10, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 11, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 12, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 32 COMPLETE: RETURN WAS 0.282429536481

TOTAL STEPS: 411, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 0.0010504486010321622, ((4, 4), 'south'): 8.078028034687044, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.6792362799995614, ((4, 2), 'exit'): 9.999961853027344, ((2, 4), 'east'): 6.010306260280609, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.011955184679764367, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.142364387989045, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.2224351258160992, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.001346728975682259, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 4.542452176643372, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 2.7249995901987076, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.998970031738281}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 33

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 33 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 417, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.0010504486010321622, ((4, 4), 'south'): 8.08855053162575, ((0, 2), 'south'): 0.0007182554536972048, ((0, 2), 'west'): 0.008768184735062027, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 0.6792362799995614, ((4, 2), 'exit'): 9.999980926513672, ((2, 4), 'east'): 6.219217104735375, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.011955184679764367, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.206294809603692, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.2224351258160992, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.001346728975682259, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 4.9758639054479605, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.0015961232304382328, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.0, ((0, 4), 'east'): 2.7249995901987076, ((1, 1), 'north'): 0.0, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999467849731445}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 34

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 8, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 9, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 10, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 18, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 19, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 20, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 21, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 22, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 23, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 24, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 25, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 26, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 27, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 28, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 29, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 30, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 31, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 32, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 33, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 34, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 35, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 36, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 37, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 38, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 39, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 40, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 41, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 42, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 34 COMPLETE: RETURN WAS 0.119725151826

TOTAL STEPS: 460, EPISODE STEPS: 43
Learned QValue: {((0, 1), 'east'): 0.29667614486211746, ((4, 4), 'south'): 8.094035798192024, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 1.565867955589199, ((4, 2), 'exit'): 9.999990463256836, ((2, 4), 'east'): 6.352441216689349, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.242995144033433, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.2596792173574721, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.2865796498548985, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 3.601638552550936, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999725341796875}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 35

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 35 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 464, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.29667614486211746, ((4, 4), 'south'): 8.096894302904605, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 1.565867955589199, ((4, 2), 'exit'): 9.999995231628418, ((2, 4), 'east'): 6.352441216689349, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2638136812031275, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.2596792173574721, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.2865796498548985, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 3.601638552550936, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999858379364014}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 36

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 36 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 466, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.29667614486211746, ((4, 4), 'south'): 8.096894302904605, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 1.565867955589199, ((4, 2), 'exit'): 9.999997615814209, ((2, 4), 'east'): 6.352441216689349, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2638136812031275, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.2596792173574721, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.2865796498548985, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 6.15234375, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 3.601638552550936, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999927043914795}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 37

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 37 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 468, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.29667614486211746, ((4, 4), 'south'): 8.096894302904605, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 1.565867955589199, ((4, 2), 'exit'): 9.999998807907104, ((2, 4), 'east'): 6.352441216689349, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2638136812031275, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.2596792173574721, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.2865796498548985, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 7.576170802116394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 3.601638552550936, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999927043914795}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 38

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 38 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 470, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.29667614486211746, ((4, 4), 'south'): 8.096894302904605, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 1.565867955589199, ((4, 2), 'exit'): 9.999999403953552, ((2, 4), 'east'): 6.352441216689349, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2638136812031275, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.2596792173574721, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.2865796498548985, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.288084864616394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 3.601638552550936, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999927043914795}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 39

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 39 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 478, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.29667614486211746, ((4, 4), 'south'): 8.09841432121396, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 2.4036713264425207, ((4, 2), 'exit'): 9.999999701976776, ((2, 4), 'east'): 6.444936764886082, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.275509276908636, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.2596792173574721, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.501888372437657, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.288084864616394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 4.179780118710172, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999963253736496}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 40

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 40 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 485, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.29667614486211746, ((4, 4), 'south'): 8.099190624788402, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.1771875, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 2.4036713264425207, ((4, 2), 'exit'): 9.999999850988388, ((2, 4), 'east'): 6.496447557051927, ((0, 1), 'south'): -2.25, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2820410830006, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.2596792173574721, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -7.5, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.651165730417565, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.288084864616394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.21926953124999998, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 4.565739826952032, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999981492757797}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 41

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: west, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 1), A: south, S': (0, 0), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 0), A: exit, S': TERMINAL_STATE, R: -10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 41 COMPLETE: RETURN WAS -5.31441

TOTAL STEPS: 492, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099190624788402, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 2.4036713264425207, ((4, 2), 'exit'): 9.999999850988388, ((2, 4), 'east'): 6.496447557051927, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2820410830006, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.651165730417565, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.288084864616394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.25913671875, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 4.565739826952032, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999981492757797}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 42

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 42 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 499, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099586984135211, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 2.4036713264425207, ((4, 2), 'exit'): 9.999999925494194, ((2, 4), 'east'): 6.525142265876234, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.285656322655081, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.74898426588215, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.288084864616394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.25913671875, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 4.82589449216392, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999990679323673}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 43

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 43 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 507, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099789297763259, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.3734881846950246, ((4, 2), 'exit'): 9.999999962747097, ((2, 4), 'east'): 6.541116478132903, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2876423041883855, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.81080615258538, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.288084864616394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.25913671875, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 4.999990165728928, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999995306134224}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 44

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 44 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 514, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099892536642031, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.3734881846950246, ((4, 2), 'exit'): 9.999999981373549, ((2, 4), 'east'): 6.549997275951225, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2887263360876595, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.848905491452497, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.288084864616394, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.25913671875, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.114857851527885, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999997636303306}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 45

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 45 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 516, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099892536642031, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.3734881846950246, ((4, 2), 'exit'): 9.999999990686774, ((2, 4), 'east'): 6.549997275951225, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2887263360876595, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 3.917204296875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.848905491452497, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.644042423926294, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.25913671875, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.114857851527885, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999997636303306}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 46

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 46 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 519, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099892536642031, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.1882276489532751, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.3734881846950246, ((4, 2), 'exit'): 9.999999995343387, ((2, 4), 'east'): 6.549997275951225, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.18842683644981897, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2887263360876595, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 0.41687388890785226, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.848905491452497, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.644042423926294, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.25913671875, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.114857851527885, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999998813960701}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 47

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: east, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 47 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 530, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099945734603331, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.999999997671694, ((2, 4), 'east'): 6.5549254892150595, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.96875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289314809532744, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.644042423926294, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.3322265625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.25913671875, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.149501953125, ((2, 1), 'north'): 0.61875, ((4, 3), 'south'): 8.999999404884875}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 48

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 48 COMPLETE: RETURN WAS 0.59049

TOTAL STEPS: 536, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099945734603331, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.999999997671694, ((2, 4), 'east'): 6.5549254892150595, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289314809532744, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.644042423926294, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999404884875}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 49

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 49 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 538, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099945734603331, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.999999998835847, ((2, 4), 'east'): 6.5549254892150595, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289314809532744, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.822021210915409, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999404884875}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 50

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 50 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 540, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099945734603331, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.999999999417923, ((2, 4), 'east'): 6.5549254892150595, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289314809532744, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 1.1324697363967897, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.911010604933836, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999404884875}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 51

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 51 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 546, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.09997259949986, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.999999999708962, ((2, 4), 'east'): 6.557654408897265, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289632985337871, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.911010604933836, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999702180503}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 52

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 52 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 549, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099986165731156, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.99999999985448, ((2, 4), 'east'): 6.557654408897265, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289632985337871, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.911010604933836, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999850959284}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 53

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 53 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 551, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099986165731156, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.99999999992724, ((2, 4), 'east'): 6.557654408897265, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289632985337871, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.911010604933836, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999925414158}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 54

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 54 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 556, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.09999304930195, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.99999999996362, ((2, 4), 'east'): 6.559162047850674, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289810267247956, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.911010604933836, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999962674337}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 55

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 55 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 558, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.09999304930195, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.99999999998181, ((2, 4), 'east'): 6.559162047850674, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289810267247956, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8719515199043, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.955505302450547, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.189436396917566, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999962674337}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 56

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 56 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 565, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099996507854428, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.999999999990905, ((2, 4), 'east'): 6.559995644186918, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289902005809855, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8875986814849535, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.955505302450547, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.237096382415718, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999981328983}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 57

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 57 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 567, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099996507854428, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 3.9884301255350607, ((4, 2), 'exit'): 9.999999999995453, ((2, 4), 'east'): 6.559995644186918, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289902005809855, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 1.7265066275666872, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.8875986814849535, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.97775265122118, ((0, 1), 'north'): 0.18777939510923608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 0.07383784295357668, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.21003571902126073, ((0, 4), 'east'): 5.237096382415718, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999981328983}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 58

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 58 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 580, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099998245525256, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.350908434854603, ((4, 2), 'exit'): 9.999999999997726, ((2, 4), 'east'): 6.5604537247078945, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28994943143942, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.89579738062659, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.97775265122118, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.267967597876089, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999990662445}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 59

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 59 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 588, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999118560728, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999998863, ((2, 4), 'east'): 6.560704106501687, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289973926206075, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.97775265122118, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.9999999953302}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 60

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 60 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 593, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999557178954, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999432, ((2, 4), 'east'): 6.560840320043577, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289986566455365, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 3.5159513383451717, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.97775265122118, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999997664588}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 61

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 61 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 599, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999777538542, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999716, ((2, 4), 'east'): 6.560914114926703, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289993083958212, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.97775265122118, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999998832038}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 62

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 62 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 604, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999888243689, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999858, ((2, 4), 'east'): 6.560953945244547, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999644187145, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.97775265122118, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999999415891}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 63

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 63 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 606, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999888243689, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999929, ((2, 4), 'east'): 6.560953945244547, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999644187145, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.97775265122118, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999999707882}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 64

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 64 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 608, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999888243689, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999964, ((2, 4), 'east'): 6.560953945244547, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999644187145, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 6.0086010847739875, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.988876325610558, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999999707882}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 65

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 65 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 611, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999888243689, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999982, ((2, 4), 'east'): 6.560953945244547, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999644187145, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.988876325610558, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999999853925}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 66

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 66 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 613, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999888243689, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.36703125000000003, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.0, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999991, ((2, 4), 'east'): 6.560953945244547, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999644187145, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.988876325610558, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.0, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 0.0, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999999926954}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Action 'west' at state (3, 1) is not optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 67

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (3, 1), A: north, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (3, 1), A: west, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (3, 1), A: west, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 67 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 621, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999888243689, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999996, ((2, 4), 'east'): 6.560953945244547, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.984375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999644187145, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.994438162805276, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.500712890625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.32961621093750004, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 4.044994346524751, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.7453125, ((4, 3), 'south'): 8.999999999926954}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 68

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 68 COMPLETE: RETURN WAS 0.729

TOTAL STEPS: 625, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999888243689, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999996, ((2, 4), 'east'): 6.560953945244547, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999644187145, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.900102866431848, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.994438162805276, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 0.3213316455235291, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 4.044994346524751, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.28709262022001, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999926954}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 69

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 69 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 633, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999944088975, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 9.999999999999998, ((2, 4), 'east'): 6.560975371464426, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289998170645385, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90248070857597, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.994438162805276, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 4.044994346524751, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.298592600004337, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999963475}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 70

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 70 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 636, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999944088975, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560975371464426, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289998170645385, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90248070857597, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.997219081402637, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 6.06999434652475, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.298592600004337, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999963475}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 71

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 71 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 640, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999972028051, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560975371464426, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999060162732, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90248070857597, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.997219081402637, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 6.06999434652475, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.298592600004337, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999981737}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 72

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 72 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 642, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999972028051, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560975371464426, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999060162732, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90248070857597, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.998609540701318, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 6.06999434652475, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.298592600004337, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999981737}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 73

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 73 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 645, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999986005807, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560975371464426, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999060162732, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.0543005422555405, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90248070857597, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.998609540701318, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 6.06999434652475, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.298592600004337, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.99999999999087}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 74

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 74 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 648, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999986005807, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560975371464426, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999060162732, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90248070857597, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.998609540701318, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 6.06999434652475, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.298592600004337, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999995435}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 75

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 75 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 655, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.09999999300085, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560987262805442, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999952378398, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.903679271446977, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.998609540701318, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 6.06999434652475, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3054126188613555, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999997717}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 76

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 76 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 658, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.09999999300085, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560987262805442, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999952378398, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 4.710353813192196, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.903679271446977, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3054126188613555, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999997717}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 77

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 77 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 664, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999996499399, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.546039636471542, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560993417105513, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999758742372, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.903679271446977, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3054126188613555, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.99999999999886}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 78

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 78 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 672, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999998249187, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.660455496723381, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560996599986824, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999877795916, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90428667342097, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.309361981581818, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.99999999999943}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 79

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 79 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 676, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999999124336, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.660455496723381, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560996599986824, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9921875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999938110093, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90428667342097, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.5857470703125001, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.309361981581818, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.815625, ((4, 3), 'south'): 8.999999999999716}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 80

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 80 COMPLETE: RETURN WAS 0.81

TOTAL STEPS: 679, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999999124336, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.660455496723381, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560996599986824, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99609375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999938110093, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.1238169916278378, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90428667342097, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 0.8708176799596273, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.6599047851562501, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.309361981581818, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.854296875, ((4, 3), 'south'): 8.999999999999716}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 81

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 81 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 689, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.09999999956204, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7194406400735085, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560998272142953, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99609375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999686609975, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.6591134693394407, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9045918067045555, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 1.8411264862123407, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.6599047851562501, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.311609993830345, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.854296875, ((4, 3), 'south'): 8.999999999999858}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 82

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 82 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 696, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999999780955, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7194406400735085, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999121968925, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99609375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999984133417, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.6591134693394407, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904745125816607, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 1.8411264862123407, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.6599047851562501, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3128713099322225, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.854296875, ((4, 3), 'south'): 8.999999999999929}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 83

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 83 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 700, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999999890446, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7194406400735085, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999121968925, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99609375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999991968139, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.6591134693394407, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904745125816607, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 1.8411264862123407, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.6599047851562501, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3128713099322225, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.854296875, ((4, 3), 'south'): 8.999999999999964}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 84

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 84 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 704, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999999945208, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7194406400735085, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999121968925, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99609375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999999593477, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.6591134693394407, ((2, 3), 'north'): 5.3076211748585465, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904745125816607, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 1.8411264862123407, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.6599047851562501, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3128713099322225, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.854296875, ((4, 3), 'south'): 8.999999999999982}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 85

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 85 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 710, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999999972596, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7194406400735085, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999559155109, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99609375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999997942729, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.6591134693394407, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904745125816607, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 1.8411264862123407, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.6599047851562501, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3128713099322225, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.854296875, ((4, 3), 'south'): 8.999999999999991}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 86

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 86 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 715, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.2978400255560587, ((4, 4), 'south'): 8.099999999986295, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7194406400735085, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999778651782, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99609375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999998959033, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.6591134693394407, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 0.26569994843357087, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904745125816607, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 1.8411264862123407, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.6599047851562501, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3128713099322225, ((1, 1), 'north'): 0.2242529296875, ((2, 1), 'north'): 0.854296875, ((4, 3), 'south'): 8.999999999999996}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 87

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 87 COMPLETE: RETURN WAS 0.4782969

TOTAL STEPS: 723, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999986295, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7194406400735085, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999778651782, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.998046875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999998959033, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.6591134693394407, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.3091853653017227, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904745125816607, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 1.8411264862123407, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.714385986328125, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.3901289062500001, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.3128713099322225, ((1, 1), 'north'): 0.4090836181640625, ((2, 1), 'north'): 0.875390625, ((4, 3), 'south'): 8.999999999999996}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 88

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 88 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 735, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999993146, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.750512409506254, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999888857456, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.998046875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999473349, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.953305022702799, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904822463301605, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.714385986328125, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.7841978675107752, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.313570961583585, ((1, 1), 'north'): 0.4090836181640625, ((2, 1), 'north'): 0.875390625, ((4, 3), 'south'): 8.999999999999998}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 89

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 89 COMPLETE: RETURN WAS 0.729

TOTAL STEPS: 739, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999993146, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.750512409506254, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999888857456, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9990234375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999473349, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.953305022702799, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904822463301605, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.7511187744140625, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 0.7841978675107752, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.313570961583585, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.88681640625, ((4, 3), 'south'): 8.999999999999998}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'north' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 90

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 1), A: west, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 90 COMPLETE: RETURN WAS 0.729

TOTAL STEPS: 743, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999993146, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.750512409506254, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999888857456, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999473349, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.953305022702799, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904822463301605, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.313570961583585, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 8.999999999999998}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 91

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 91 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 746, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999996573, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.750512409506254, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999888857456, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999473349, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.953305022702799, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904822463301605, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.313570961583585, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 92

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 92 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 751, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999998285, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.750512409506254, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999944191735, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999735133, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.953305022702799, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904822463301605, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.313570961583585, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 93

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 93 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 756, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999142, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.750512409506254, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999971976678, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999998667955, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 3.953305022702799, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904822463301605, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.313570961583585, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 94

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 94 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 765, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999572, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.76636313746574, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999985928397, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999933011, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904861219040308, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.313955589277515, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 95

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 95 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 773, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999785, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999992934054, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999966312, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 96

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 96 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 775, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999785, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999992934054, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999966312, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 97

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 97 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 780, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999891, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999964518675, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999999998306, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.60626019231529, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 98

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.772727
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.772727
--------------------------------
EPISODE 98 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 786, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999945, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 0.3844335937500001, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999982183105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999999914815, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.755580094560985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999304770350658, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.084371466577968, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Action 'west' at state (2, 1) is not optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.772727
################################

BEGINNING EPISODE: 99

Current Policy Convergence Ratio: 0.772727
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 99 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 790, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999945, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999982183105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999999914815, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.755580094560985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.577150271123662, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99965238517533, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.591872879946781, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 100

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 100 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 793, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999945, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999982183105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999999914815, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.755580094560985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.838575135561831, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99965238517533, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.591872879946781, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 101

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 101 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 796, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999945, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999982183105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999999914815, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.755580094560985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99965238517533, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.591872879946781, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 102

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 102 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 799, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999973, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999982183105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999999914815, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.755580094560985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99965238517533, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.591872879946781, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 103

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 103 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 802, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999973, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999982183105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.2899999999914815, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.755580094560985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999826192587665, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.845780013302289, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 104

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 104 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 808, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999987, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.774461583907752, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999105322, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999995729, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.114383095629214, ((2, 3), 'north'): 5.830240046478732, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904880603187933, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999826192587665, ((0, 1), 'north'): 2.5671643043089185, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.845780013302289, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314165343206897, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 105

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 105 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 818, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999994, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.778605196396979, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999550739, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999997859, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.205699260573096, ((2, 3), 'north'): 5.830240046478732, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904890301191362, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999826192587665, ((0, 1), 'north'): 3.1350545451876055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.845780013302289, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314278943038018, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 106

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 106 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 822, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.778605196396979, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999550739, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999998926, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.205699260573096, ((2, 3), 'north'): 5.830240046478732, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904890301191362, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999826192587665, ((0, 1), 'north'): 3.1350545451876055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.845780013302289, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314278943038018, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 107

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 107 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 824, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.778605196396979, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999550739, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999998926, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.205699260573096, ((2, 3), 'north'): 5.830240046478732, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904890301191362, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999913096293833, ((0, 1), 'north'): 3.1350545451876055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.845780013302289, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314278943038018, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 108

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 108 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 830, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.778605196396979, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999774886, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999463, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.205699260573096, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904890301191362, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999913096293833, ((0, 1), 'north'): 3.1350545451876055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.845780013302289, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314278943038018, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 109

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 109 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 838, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 3.3801839568350855, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.780728122565598, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999887201, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999731, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.205699260573096, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904895150494379, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999913096293833, ((0, 1), 'north'): 3.1350545451876055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.845780013302289, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314340107055122, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 110

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 110 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 842, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.780728122565598, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999887201, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999731, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.205699260573096, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904895150494379, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999956548146915, ((0, 1), 'north'): 3.1350545451876055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.972850899983369, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314340107055122, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 111

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 111 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 844, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.780728122565598, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999887201, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999731, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.205699260573096, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 1.4830996014464146, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904895150494379, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999956548146915, ((0, 1), 'north'): 3.1350545451876055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 1.042019280484171, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.972850899983369, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314340107055122, ((1, 1), 'north'): 0.8719366297329179, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 112

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 112 COMPLETE: RETURN WAS 1.85302018885

TOTAL STEPS: 861, EPISODE STEPS: 17
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.781817109457604, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.56099999994348, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999864, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.254177285441067, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904897575196431, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999956548146915, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.972850899983369, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314389253347487, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 113

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 113 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 867, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.781817109457604, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999999716795, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999932, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.254177285441067, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904898787572781, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999956548146915, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 7.972850899983369, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314389253347487, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 114

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 114 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 870, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.781817109457604, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999999716795, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999932, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.254177285441067, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904898787572781, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999978274073458, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.036405896657797, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314389253347487, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 115

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 115 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 875, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.781817109457604, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999985809, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999965, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.254177285441067, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904898787572781, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999978274073458, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.036405896657797, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314389253347487, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 116

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 116 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 884, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782383718735171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999992889, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999981, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.278906341976455, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 7.969287567780915, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048993937800045, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999978274073458, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.036405896657797, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314399081081495, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 117

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 117 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 887, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782383718735171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999992889, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999981, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.278906341976455, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048993937800045, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999978274073458, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.036405896657797, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314399081081495, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 118

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 118 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 889, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782383718735171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999992889, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999981, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.278906341976455, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048993937800045, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999989137036728, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.036405896657797, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314399081081495, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 119

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 119 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 892, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782383718735171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999992889, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999981, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.278906341976455, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048993937800045, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999994568518364, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.068198059995426, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314399081081495, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 120

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 120 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 895, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 0.28170707448517107, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782383718735171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999992889, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999981, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.278906341976455, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.15232434605763, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048993937800045, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999994568518364, ((0, 1), 'north'): 3.460091939851696, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.068198059995426, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314399081081495, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 121

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 121 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 907, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782671445854259, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999996437, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999999999999, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899696886803, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999994568518364, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.068198059995426, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.31440426774175, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 122

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 122 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 910, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782671445854259, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999996437, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.28999999999999, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899696886803, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999997284259182, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.084096585830977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.31440426774175, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 123

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 123 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 915, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782671445854259, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999998214, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999994, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899696886803, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999997284259182, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.084096585830977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.31440426774175, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 124

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 124 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 918, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782671445854259, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999998214, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999994, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.8675700230371985, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899696886803, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999997284259182, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.084096585830977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.31440426774175, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 125

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 125 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 924, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782671445854259, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999996, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899696886803, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999997284259182, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.084096585830977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.31440426774175, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 126

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 126 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 926, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782671445854259, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999996, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899696886803, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999864212959, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.084096585830977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.31440426774175, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 127

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 127 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 929, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782671445854259, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999105, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999996, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899696886803, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999321064795, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.092047681873805, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.31440426774175, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 128

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 128 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 937, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782817643410917, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.5609999999995505, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899848442998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999321064795, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.092047681873805, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314406997469936, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 129

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 129 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 942, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782817643410917, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999774, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899848442998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999321064795, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.092047681873805, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314406997469936, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 130

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 130 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 948, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782817643410917, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999886, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999242213975, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999321064795, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.092047681873805, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314406997469936, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 131

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 131 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 950, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782817643410917, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999886, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999242213975, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999660532398, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.092047681873805, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314406997469936, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 132

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 132 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 955, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782817643410917, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999942, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.291525844419055, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.633203545962078, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999242213975, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999660532398, ((0, 1), 'north'): 3.6555538238152527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.092047681873805, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 0.23904587101085678, ((0, 4), 'east'): 5.314406997469936, ((1, 1), 'north'): 1.30586132807872, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 133

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 133 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 971, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782891970566929, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.56099999999997, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.29803086174444, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899962110672, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999660532398, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.092047681873805, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314408464634597, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 134

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 134 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 974, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782891970566929, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.56099999999997, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.29803086174444, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899962110672, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999830266198, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314408464634597, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 135

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 135 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 977, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782891970566929, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.56099999999997, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.29803086174444, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899962110672, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999830266198, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314408464634597, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 136

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 136 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 986, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782929794369034, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999984, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.301316817627338, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899981055323, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999830266198, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409215267101, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 137

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 137 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 995, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782949044054712, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999991, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.302976816279735, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899990527654, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999830266198, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409599108446, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 138

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 138 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 997, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782949044054712, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999991, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.302976816279735, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899990527654, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999915133099, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409599108446, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 139

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 139 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1002, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782949044054712, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999995, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.302976816279735, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899990527654, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999915133099, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409599108446, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 140

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 140 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1008, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782949044054712, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999996, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.302976816279735, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899995263825, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999915133099, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409599108446, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 141

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 141 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1010, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782949044054712, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999996, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.302976816279735, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899995263825, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999995756655, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 2.9124774323045997, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409599108446, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 142

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 142 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 1021, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782958841626157, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999997, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.303815477964488, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899997631911, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999995756655, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409896580193, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 143

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 143 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1027, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782958841626157, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.303815477964488, ((2, 3), 'north'): 5.886235011517796, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899998815955, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999995756655, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409896580193, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 144

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 144 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1033, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782958841626157, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.303815477964488, ((2, 3), 'north'): 5.895567505758898, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899998815955, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999995756655, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409896580193, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 145

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 145 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 1042, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782963874274165, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304239217714015, ((2, 3), 'north'): 5.895567505758898, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999407977, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999995756655, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409947757277, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 146

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 146 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1050, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782966413627857, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304239217714015, ((2, 3), 'north'): 5.895567505758898, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999997039875, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999995756655, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09602368817648, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099736122286, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 147

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 147 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1053, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782966413627857, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304239217714015, ((2, 3), 'north'): 5.895567505758898, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999997039875, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999978783276, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099736122286, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 148

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 148 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1055, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 0.8798987217025662, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782966413627857, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304239217714015, ((2, 3), 'north'): 5.895567505758898, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 2.961600993697903, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999997039875, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999989391638, ((0, 1), 'north'): 3.7589635418962013, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099736122286, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 149

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 149 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 1068, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829676949394315, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.895567505758898, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999998519935, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999989391638, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409986672908, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 150

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 150 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1074, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829676949394315, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999998519935, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999989391638, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409986672908, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 151

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 151 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1076, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829676949394315, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999998519935, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409986672908, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 152

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 152 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1079, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829676949394315, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999998519935, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409986672908, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 153

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 153 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1087, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968341472524, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999925997, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409993269852, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 154

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 154 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1093, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968341472524, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999962998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409993269852, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 155

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 155 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1097, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968341472524, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999962998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409993269852, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 156

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 156 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1100, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968341472524, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999962998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409993269852, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 157

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 157 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 1107, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968341472524, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304454494989543, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999981499, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8163894189194076, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409996618275, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 158

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 158 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1117, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968669214487, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304563001157408, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999990748, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.8451992322049984, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409998300812, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 159

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 159 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1127, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999995374, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999469582, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 160

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 160 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1129, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999995374, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999734791, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 161

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 161 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1132, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999995374, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999734791, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 162

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 162 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1134, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999995374, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999734791, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.098011824993186, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 163

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 163 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1137, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.900233752879448, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999995374, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999998673955, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099005911303152, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 164

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 164 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1143, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999995374, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999998673955, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099005911303152, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 165

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 165 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1149, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999997687, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999998673955, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099005911303152, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 166

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 166 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1152, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829688338426095, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999997687, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999998673955, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099005911303152, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999146243, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 167

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 167 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1160, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968916537114, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.034643783890457, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999998843, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999998673955, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099005911303152, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999572081, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 168

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 168 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1163, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968916537114, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999998843, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999998673955, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099005911303152, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999572081, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 169

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 169 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1165, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 5.2206929844035725, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968916537114, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999998843, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999336978, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099005911303152, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999572081, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 170

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 170 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1169, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.254899152288205, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968916537114, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999998843, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999966849, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099502955353216, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999572081, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 171

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 171 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1172, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.254899152288205, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968916537114, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999998843, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999834245, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099751477527429, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999572081, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 172

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 172 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1176, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.0663613911319905, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.772337741031445, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968916537114, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304617401725223, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999998843, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999917122, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099875738689125, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999572081, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 173

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 173 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1186, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.970258526342346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.772337741031445, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968958075994, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304644713304313, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.9048999999994205, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999917122, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099875738689125, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.31440999978552, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 174

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 174 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1192, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.970258526342346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.772337741031445, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968958075994, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304644713304313, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90489999999971, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999917122, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099875738689125, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.31440999978552, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 175

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 175 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1195, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.970258526342346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.772337741031445, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968958075994, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304644713304313, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90489999999971, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999917122, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099875738689125, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.31440999978552, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 176

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 176 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 1204, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 2.970258526342346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.772337741031445, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968978941481, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304658387786354, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999854, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999917122, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099875738689125, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099998926295, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 177

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 177 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1214, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.772337741031445, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999927, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999917122, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099875738689125, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.31440999994625, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 178

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 178 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1220, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 6.772337741031445, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999962, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999917122, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099875738689125, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.31440999994625, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 179

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 179 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1224, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.031112952925829, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.902566876439723, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999962, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999958561, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099937869307269, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.31440999994625, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 180

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 180 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1230, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.031112952925829, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999962, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999958561, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 3.8477231249926245, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099937869307269, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.31440999994625, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 181

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 181 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 1239, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.031112952925829, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90489999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999958561, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.3153460624842115, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099937869307269, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999865375, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 182

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 182 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1243, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90489999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999997928, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.3153460624842115, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099968934634987, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999865375, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 183

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 183 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1246, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90489999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.3153460624842115, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999865375, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 184

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 184 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1251, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.90489999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.3153460624842115, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999865375, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 185

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 185 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 1258, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968989422423, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304665234416843, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.277700639204412, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999989, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.859652966623333, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.3153460624842115, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999993259, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 186

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 186 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 1269, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968994708178, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304668662448512, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.375694154582706, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999994, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.8669258387992462, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.3153460624842115, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999996625, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 187

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 187 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1279, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968997352571, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304668662448512, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.375694154582706, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999996, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.8669258387992462, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.549157531241345, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999152, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 188

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 188 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1284, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.4222255376750326, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968997352571, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304668662448512, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.375694154582706, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999996, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.8669258387992462, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.549157531241345, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999152, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 189

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 189 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 1297, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.7612077315715045, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968998675904, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304670380032913, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.427963704751014, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.870563817501454, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.549157531241345, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999574, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 190

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 190 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1307, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.7612077315715045, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.78296899933776, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671239420614, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.427963704751014, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.872383579765538, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.549157531241345, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999786, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 191

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 191 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 1321, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.160528517651185, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999668784, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671669412299, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.427963704751014, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999998964, ((0, 1), 'north'): 3.8732938476220453, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09998446730817, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999999455, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 192

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 192 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1325, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999668784, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671669412299, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.427963704751014, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999482, ((0, 1), 'north'): 3.8732938476220453, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099992233649424, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999999455, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 193

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 193 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1327, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999668784, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671669412299, ((2, 3), 'north'): 5.903733438219861, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.427963704751014, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999482, ((0, 1), 'north'): 3.8732938476220453, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099992233649424, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999999455, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 194

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 194 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1333, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999668784, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671669412299, ((2, 3), 'north'): 5.90431671910993, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.427963704751014, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999482, ((0, 1), 'north'): 3.8732938476220453, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099992233649424, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999999455, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 195

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 195 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1339, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 0.21174163085564968, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999668784, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671669412299, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.427963704751014, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999482, ((0, 1), 'north'): 3.8732938476220453, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099992233649424, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.3144099999999455, ((1, 1), 'north'): 2.3033818249104234, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 196

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 196 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 1353, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999834367, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671884557102, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999482, ((0, 1), 'north'): 3.873976838758813, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099992233649424, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999972, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 197

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 197 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1355, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999834367, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671884557102, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999741, ((0, 1), 'north'): 3.873976838758813, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099992233649424, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999972, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 198

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 198 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1358, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999834367, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671884557102, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999998705, ((0, 1), 'north'): 3.873976838758813, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099996116823547, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999972, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 199

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 199 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1363, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999834367, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671884557102, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999998705, ((0, 1), 'north'): 3.873976838758813, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099996116823547, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999972, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 200

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 200 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1366, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.22525726911427, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999834367, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671884557102, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999998705, ((0, 1), 'north'): 3.873976838758813, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099996116823547, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999972, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 201

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 201 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1370, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999834367, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671884557102, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999353, ((0, 1), 'north'): 3.873976838758813, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.666063265620624, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099998058411192, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999972, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 202

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 202 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 1382, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999353, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7245161328103045, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099998058411192, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999992, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 203

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 203 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1385, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999353, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7245161328103045, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099998058411192, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999992, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 204

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 204 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1389, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999353, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7245161328103045, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099998058411192, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999992, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 205

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 205 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1392, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999677, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7245161328103045, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999992, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 206

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 206 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1394, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7245161328103045, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999992, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 207

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 207 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1399, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7245161328103045, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999992, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 208

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 208 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 1413, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7793156958006415, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 209

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 209 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1416, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7793156958006415, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 210

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 210 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1421, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8459550195017904, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999917171, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304671992204016, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4569640838054276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8740907674301024, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7793156958006415, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 2.694274579593168, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 211

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 18, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 211 COMPLETE: RETURN WAS 1.50094635297

TOTAL STEPS: 1440, EPISODE STEPS: 19
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8706735712983864, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999958584, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672046064734, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.47182288724626, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.874147780206858, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.781142347900319, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 212

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 212 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 1452, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.781142347900319, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 213

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 213 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1454, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.781142347900319, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 214

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 214 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1457, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999838, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.781142347900319, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 215

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 215 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1459, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999918, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.781142347900319, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 216

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 216 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1467, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999918, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999029205305, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 217

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 217 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1470, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999996, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999514602615, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 218

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 218 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1473, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904608359554965, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 219

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 219 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1479, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904754179777482, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 220

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 220 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1483, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904754179777482, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 221

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 221 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1489, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8724392063783237, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999979291, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.30467207301373, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.479277944716216, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8741763108325595, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.0591435384459205, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 222

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 222 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 1503, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8737634508788488, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999989644, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672086497546, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8741905882724583, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 223

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 223 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1505, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 1.8490579441987753, ((0, 2), 'west'): 3.8737634508788488, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999989644, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672086497546, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8741905882724583, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 224

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 224 COMPLETE: RETURN WAS 1.85302018885

TOTAL STEPS: 1522, EPISODE STEPS: 17
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999948215, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.067321891945229, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 225

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 225 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1525, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999948215, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.083660945972614, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 226

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 226 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 1532, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999948215, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.083660945972614, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 227

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 227 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1536, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999948215, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.083660945972614, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 228

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 228 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1541, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999948215, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.083660945972614, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 229

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 229 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1547, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999948215, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.083660945972614, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 230

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 230 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1555, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.78296899999741, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.083660945972614, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 231

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 231 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1558, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.78296899999741, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672093244113, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.8742013054539584, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 232

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 232 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1568, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999998704, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672096620891, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999979, ((0, 1), 'north'): 3.87420309468683, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 233

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 233 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1570, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999998704, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672096620891, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.48301831223276, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.87420309468683, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.095246844345257, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 234

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 234 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 1582, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999351, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672098309862, ((2, 3), 'north'): 5.904827089888741, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4849005487254536, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.874203990822816, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.114981662677371, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 235

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 235 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1588, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999351, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672098309862, ((2, 3), 'north'): 5.904863544944369, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4849005487254536, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.874203990822816, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.114981662677371, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 236

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 236 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1598, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999996745, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099154638, ((2, 3), 'north'): 5.904863544944369, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4849005487254536, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.8742044396508457, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.114981662677371, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 237

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 237 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1602, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999996745, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099154638, ((2, 3), 'north'): 5.904863544944369, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4849005487254536, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.8742044396508457, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.114981662677371, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 238

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 238 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1606, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999996745, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099154638, ((2, 3), 'north'): 5.904863544944369, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4849005487254536, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.8742044396508457, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.114981662677371, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 239

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 239 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1611, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741772886622328, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.7829689999996745, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099154638, ((2, 3), 'north'): 5.904863544944369, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4849005487254536, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.8742044396508457, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 2.5398575018607694, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.114981662677371, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 240

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 240 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 1627, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741910889507034, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999836, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099577173, ((2, 3), 'north'): 5.904863544944369, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4858422722056073, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.87420466444501, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 3.6614132509303836, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 241

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 241 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1633, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741910889507034, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999836, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099577173, ((2, 3), 'north'): 5.904881772472184, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4858422722056073, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.99999999999999, ((0, 1), 'north'): 3.87420466444501, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 3.6614132509303836, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999975730129, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 242

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 242 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1636, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741910889507034, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999836, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099577173, ((2, 3), 'north'): 5.904881772472184, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4858422722056073, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999995, ((0, 1), 'north'): 3.87420466444501, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 3.6614132509303836, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999987865064, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 243

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 243 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1646, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 1.8674997016672923, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8741910889507034, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999917, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.3046720997885135, ((2, 3), 'north'): 5.904881772472184, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4858422722056073, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999995, ((0, 1), 'north'): 3.87420466444501, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.222191125465191, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999987865064, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 244

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 244 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 1661, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8742014395949225, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999957, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099894219, ((2, 3), 'north'): 5.904881772472184, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999995, ((0, 1), 'north'): 3.874204777127336, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.502580062732594, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999987865064, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 245

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 245 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1664, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8742014395949225, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999957, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099894219, ((2, 3), 'north'): 5.904881772472184, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999995, ((0, 1), 'north'): 3.874204777127336, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.502580062732594, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999987865064, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 246

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 246 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1674, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8742014395949225, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999977, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099947091, ((2, 3), 'north'): 5.904881772472184, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999995, ((0, 1), 'north'): 3.8742048335160666, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.502580062732594, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999987865064, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 247

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 247 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 1683, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8742014395949225, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999977, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099947091, ((2, 3), 'north'): 5.904881772472184, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999995, ((0, 1), 'north'): 3.8742048335160666, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999987865064, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 248

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 248 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1689, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8742014395949225, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999977, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099947091, ((2, 3), 'north'): 5.904890886236091, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999995, ((0, 1), 'north'): 3.8742048335160666, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999987865064, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 249

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 249 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1692, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.0773463482880534, ((0, 2), 'west'): 3.8742014395949225, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999977, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099947091, ((2, 3), 'north'): 5.904890886236091, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.8742048335160666, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7820556739501585, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 250

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 250 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 1707, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874203164773652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999987, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099973535, ((2, 3), 'north'): 5.904890886236091, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782512336975078, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 251

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 251 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1709, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874203164773652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999987, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099973535, ((2, 3), 'north'): 5.904890886236091, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782512336975078, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 252

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 252 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1713, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874203164773652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 0.1771875, ((0, 3), 'north'): 4.782968999999987, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099973535, ((2, 3), 'north'): 5.904890886236091, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782512336975078, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 253

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: west, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 253 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 1720, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874203164773652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999987, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099973535, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782512336975078, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 254

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 254 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1728, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874203164773652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999987, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099973535, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 255

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 255 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1731, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874203164773652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999987, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099973535, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 256

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 256 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1741, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874204027374917, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.7829689999999925, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099986762, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 257

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 257 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1745, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874204027374917, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.7829689999999925, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099986762, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999996, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999939325318, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 258

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 258 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1748, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.5023788733261694, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.2820653619244275, ((0, 2), 'west'): 3.874204027374917, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.7829689999999925, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099986762, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486313235103058, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999998, ((0, 1), 'north'): 3.874204875843303, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999969662658, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1337318899559663, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 259

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 18, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 19, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 259 COMPLETE: RETURN WAS 1.35085171767

TOTAL STEPS: 1768, EPISODE STEPS: 20
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999995, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999998, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999969662658, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 260

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 260 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1771, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999995, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999998, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999969662658, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 261

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 261 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1777, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999995, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 8.999999999999998, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999969662658, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 262

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 262 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1780, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999995, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999984831328, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 263

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 263 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1783, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999995, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999984831328, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 264

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 264 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1788, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999995, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999984831328, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 265

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 265 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1796, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999996, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999984831328, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 266

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 266 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1804, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904895443118045, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999984831328, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 267

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 267 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1810, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904897721559022, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999984831328, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 268

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 268 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1814, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904897721559022, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999984831328, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 269

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 269 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1817, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904897721559022, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999992415665, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 270

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 270 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1819, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904897721559022, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999992415665, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 271

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 271 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1822, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904897721559022, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999996207831, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 272

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 272 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1825, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904897721559022, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.091830472986306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 273

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 273 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1828, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904897721559022, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 274

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 274 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1834, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904898860779511, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 275

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 275 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1840, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899430389754, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 276

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 276 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1843, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899430389754, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 277

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 277 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1845, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899430389754, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 278

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 278 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1851, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899715194876, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 279

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 279 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1856, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.257626887127731, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899715194876, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999998103915, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 280

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 280 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 1860, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999997, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899715194876, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 281

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 281 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1868, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899715194876, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 282

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 282 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 1875, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899715194876, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 283

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 283 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 1877, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899715194876, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 284

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 284 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1883, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.461194516332307, ((0, 2), 'west'): 3.874204674334794, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099993377, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204888219988, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 285

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 285 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 1896, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.473989457865148, ((0, 2), 'west'): 3.874204836079228, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099996688, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048891070135, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 286

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 286 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 1903, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.473989457865148, ((0, 2), 'west'): 3.874204836079228, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099996688, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048891070135, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 287

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 287 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 1910, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.473989457865148, ((0, 2), 'west'): 3.874204836079228, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099996688, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048891070135, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 288

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 288 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1916, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.473989457865148, ((0, 2), 'west'): 3.874204836079228, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099996688, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4865488116810153, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048891070135, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 289

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 289 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 1930, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4803869292309813, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099998344, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4866666059386637, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889774518, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 290

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 290 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1935, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4803869292309813, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099998344, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4866666059386637, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889774518, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.712871765683147, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.135706900774359, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 291

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 291 COMPLETE: RETURN WAS 1.66771816997

TOTAL STEPS: 1953, EPISODE STEPS: 18
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.765444691420785, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 292

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 292 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 1958, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.765444691420785, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 293

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 293 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 1964, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782740668487538, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.765444691420785, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 294

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 294 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 1974, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782854834243768, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.774206845710392, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999051956, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 295

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 295 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 1977, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782854834243768, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.774206845710392, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 296

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 296 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1985, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782854834243768, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.774206845710392, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 297

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 297 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 1993, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782911917121883, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.774206845710392, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 298

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 298 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2001, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782911917121883, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.778587922855195, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 299

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 299 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 2011, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.780778461427596, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 300

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 300 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2013, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.095915236493152, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.780778461427596, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 301

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 301 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2016, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.097957618246575, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.780778461427596, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 302

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 302 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2019, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.780778461427596, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 303

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 303 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2023, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.780778461427596, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999525977, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 304

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 304 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2026, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.8200303924594605, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048630381234, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999172, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486725503367865, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899705104, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.780778461427596, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999976299, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 305

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 305 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 2040, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.874204876518689, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.3046720999995856, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.781873730713797, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999976299, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 306

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 306 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2043, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.874204876518689, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.3046720999995856, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.781873730713797, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999881494, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 307

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 307 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2047, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.874204876518689, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.3046720999995856, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.781873730713797, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999881494, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 308

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 308 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2056, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.874204876518689, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.3046720999995856, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782421365356898, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999881494, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 309

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 309 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2059, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.874204876518689, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.273813442710628, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.3046720999995856, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782421365356898, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999940746, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 310

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 310 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2063, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.874204876518689, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2819067213286495, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.3046720999995856, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78294045856094, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782421365356898, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999970372, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 311

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 311 COMPLETE: RETURN WAS 1.66771816997

TOTAL STEPS: 2081, EPISODE STEPS: 18
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048866293928, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2819067213286495, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999792, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829518864174005, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999970372, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 312

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 312 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2085, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048866293928, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999792, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829518864174005, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999985187, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 313

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 313 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2091, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.485185033006341, ((0, 2), 'west'): 3.8742048866293928, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999792, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899848824, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829518864174005, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999985187, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 1.7645221562222921, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 314

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 314 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 2106, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4859847169997273, ((0, 2), 'west'): 3.874204889157208, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999895, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.098978809123288, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829518864174005, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999985187, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 315

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 315 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2109, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4859847169997273, ((0, 2), 'west'): 3.874204889157208, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999895, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829518864174005, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999985187, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 316

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 316 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2112, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4859847169997273, ((0, 2), 'west'): 3.874204889157208, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999895, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829518864174005, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 317

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 317 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2114, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4859847169997273, ((0, 2), 'west'): 3.874204889157208, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999895, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867549521706622, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999608, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829518864174005, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 318

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 318 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 2127, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4859847169997273, ((0, 2), 'west'): 3.8742048895785564, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999946, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486769676583567, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899979924, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782960443208699, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 319

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 319 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2131, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4859847169997273, ((0, 2), 'west'): 3.8742048895785564, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999946, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486769676583567, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899979924, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782960443208699, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 320

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 320 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2143, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4859847169997273, ((0, 2), 'west'): 3.8742048895785564, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999973, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889998972, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829647216043485, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 321

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 321 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 2156, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486384558999627, ((0, 2), 'west'): 3.874204889789266, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999985, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999725, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782954729280469, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.7829647216043485, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 322

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 322 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2168, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486384558999627, ((0, 2), 'west'): 3.874204889789266, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999992, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999725, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782961864640233, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782966860802173, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 323

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 323 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2170, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486384558999627, ((0, 2), 'west'): 3.874204889789266, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999992, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999725, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782961864640233, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782966860802173, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 324

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 324 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2174, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486384558999627, ((0, 2), 'west'): 3.874204889789266, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999992, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999725, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782961864640233, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782966860802173, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 325

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 325 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2177, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486384558999627, ((0, 2), 'west'): 3.874204889789266, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999992, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999725, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782961864640233, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782966860802173, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 326

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 326 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 2184, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486384558999627, ((0, 2), 'west'): 3.874204889789266, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 0.011677170410156252, ((0, 2), 'north'): 4.304672099999992, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999725, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782961864640233, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782966860802173, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 327

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 327 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2196, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486384558999627, ((0, 2), 'west'): 3.874204889789266, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 2.663043585205077, ((0, 2), 'north'): 4.304672099999992, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999725, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782965432320116, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782967930401085, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 328

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 328 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 2211, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48658447999975, ((0, 2), 'west'): 3.874204889973652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 2.663043585205077, ((0, 2), 'north'): 4.304672099999996, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999926, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782965432320116, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782967930401085, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 329

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 329 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2215, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48658447999975, ((0, 2), 'west'): 3.874204889973652, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 2.663043585205077, ((0, 2), 'north'): 4.304672099999996, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999926, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782965432320116, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782967930401085, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 330

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 18, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 19, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 330 COMPLETE: RETURN WAS 1.35085171767

TOTAL STEPS: 2235, EPISODE STEPS: 20
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48658447999975, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 3.988726792602537, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999926, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968108080027, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968966575032, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 331

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 331 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 2245, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 2.9790416727452698, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48658447999975, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 4.6515683963012675, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867770387908803, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999926, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968108080027, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 0.0, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968966575032, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 2.6256532786077025, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.136853423059578, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 332

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 18, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 19, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 20, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 21, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 22, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 23, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 24, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 25, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 26, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 27, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 28, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 29, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 30, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 31, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 32, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 33, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 34, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 35, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 332 COMPLETE: RETURN WAS 0.250315550499

TOTAL STEPS: 2281, EPISODE STEPS: 36
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486684440499858, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 4.982989198150633, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968966575032, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 333

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 333 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 2292, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486684440499858, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.148699599075315, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 334

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 334 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2294, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486684440499858, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.148699599075315, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 335

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 335 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2299, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486684440499858, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.148699599075315, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 336

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 336 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2302, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486684440499858, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.148699599075315, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.87420488999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 337

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 337 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 2313, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.285953360650993, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.148699599075315, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999992594, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 338

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 338 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2317, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.148699599075315, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999996296, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 339

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 339 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2325, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.148699599075315, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999996296, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 340

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 340 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2334, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899857597437, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999996296, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 341

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 341 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2340, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999996296, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 342

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 342 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2343, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999996296, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 343

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 343 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2347, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999996296, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 344

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 344 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2350, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999998147, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 345

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 345 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2355, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999998147, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 346

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 346 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2358, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2879766803221635, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999074, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 347

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 347 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2362, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968777020005, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 348

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 348 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 2372, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.231554799537657, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968983287515, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 349

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 349 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 2383, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 350

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 350 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2385, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 351

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 351 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2394, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 352

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 352 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2398, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 353

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 353 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2402, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 354

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 354 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2405, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999536, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 355

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 355 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2408, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999767, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 356

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 356 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2410, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999767, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 357

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 357 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2412, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 3.1281627863932364, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.8742048899868244, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867839408618675, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999989, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999767, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.2715016204018754, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1380953083753944, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 358

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 18, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 358 COMPLETE: RETURN WAS 1.50094635297

TOTAL STEPS: 2431, EPISODE STEPS: 19
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999767, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4329637058504616, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 359

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 359 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2439, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999767, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4329637058504616, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 360

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 360 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2442, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999884, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4329637058504616, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 361

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 361 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 2453, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999957, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999884, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 362

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 362 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2456, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099489404561645, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999957, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999884, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 363

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 363 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2459, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999957, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999884, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 364

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 364 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2462, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999957, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999941, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 365

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 365 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2467, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.48673442074992, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999957, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999941, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 366

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 366 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 2478, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999997, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999941, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 367

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 367 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2481, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999997, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999997, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 368

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 368 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 2491, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999975, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999997, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 369

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 369 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2496, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.288988340160666, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999975, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999997, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 370

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 370 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2500, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289494170080319, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999975, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999984, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 371

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 371 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2505, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289494170080319, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999975, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999984, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 372

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 372 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2509, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999975, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999999, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 373

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 373 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2517, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899928798718, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999975, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999999, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 374

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 374 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2523, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 3.135620063742225, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486759410874958, ((0, 2), 'west'): 3.874204889993411, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.272982399768827, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.486784343482725, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.8742048899999975, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968888510002, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968995821877, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999999, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 375

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 12, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 13, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 14, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 15, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 16, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 17, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 18, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 19, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 375 COMPLETE: RETURN WAS 1.35085171767

TOTAL STEPS: 2543, EPISODE STEPS: 20
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.293696199884412, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968944255, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968997910937, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999999, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 376

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 376 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 2554, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.293696199884412, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968944255, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999477733, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999999, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 377

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 377 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2560, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.293696199884412, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968944255, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999477733, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.09999999999999, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 378

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 378 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2563, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.293696199884412, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968944255, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999477733, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999994, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 379

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 5, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 379 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2575, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.309231549971102, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099744702280823, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968972127499, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999738865, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999994, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 380

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.818182
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.818182
--------------------------------
EPISODE 380 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2578, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289747085040153, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.309231549971102, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099872351140412, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968972127499, ((1, 1), 'east'): 0.8523315766898546, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999738865, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999994, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Action 'west' at state (1, 1) is not optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.818182
################################

BEGINNING EPISODE: 381

Current Policy Convergence Ratio: 0.818182
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 381 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2583, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.309231549971102, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099872351140412, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968972127499, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999738865, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.459874053425228, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 382

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 382 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 2597, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.309231549971102, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099872351140412, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999869432, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 383

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 5, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 383 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2609, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31182077498555, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099872351140412, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 384

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 384 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2611, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31182077498555, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099872351140412, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 385

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 385 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2614, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31182077498555, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099936175570207, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 386

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 386 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2623, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099936175570207, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 387

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 387 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2625, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099936175570207, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 388

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 388 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2628, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099968087785104, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 389

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 389 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2633, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099968087785104, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 390

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 390 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2636, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 391

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 391 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2641, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.486771905937478, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.473329227212613, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 392

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 392 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 2655, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867812772343685, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968986063748, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 0.015937914454719544, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999934715, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 393

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 5, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 6, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 7, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 8, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 9, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 10, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 393 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 2671, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867812772343685, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968996515935, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999991837, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 394

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 394 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2674, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867812772343685, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968996515935, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999991837, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 395

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 7, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 395 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 2689, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 3.136862986438339, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867828391171836, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289873542520073, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968998257966, ((1, 1), 'east'): 3.706551976612996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 396

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.863636
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.863636
--------------------------------
EPISODE 396 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2695, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 3.236379882695018, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867828391171836, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289936771260036, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968998257966, ((1, 1), 'east'): 5.133719082440531, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Action 'north' at state (0, 1) is not optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.863636
################################

BEGINNING EPISODE: 397

Current Policy Convergence Ratio: 0.863636
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 397 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2701, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 3.928363528445748, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867828391171836, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289968385630017, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968998257966, ((1, 1), 'east'): 5.847331088287282, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 398

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 398 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2713, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 3.928363528445748, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867828391171836, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289968385630017, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78296899956449, ((1, 1), 'east'): 5.847331088287282, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 399

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 399 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2717, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 3.928363528445748, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.4867828391171836, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289968385630017, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78296899956449, ((1, 1), 'east'): 5.847331088287282, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.4800568141063053, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 400

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 400 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2726, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.313115387492774, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78296899956449, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 401

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 401 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2734, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78296899956449, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 402

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 402 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2736, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78296899956449, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 403

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 403 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2745, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 404

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 404 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2750, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 405

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 405 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2753, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 406

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 406 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2758, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 407

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 407 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2761, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 408

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 408 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2763, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 409

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 409 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2766, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 410

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 410 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2772, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289984192815007, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.204151317677148, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 411

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 411 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2777, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289992096407502, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099984043892551, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.382568545605327, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 412

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 412 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2780, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289992096407502, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314086346873191, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999782244, ((1, 1), 'east'): 6.382568545605327, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999997958, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 413

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 413 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 2794, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 4.595480753952151, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.5111550073591786, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289992096407502, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314248173436594, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78296899994556, ((1, 1), 'east'): 6.382568545605327, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999743, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 414

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 414 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2802, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289996048203751, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314248173436594, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.78296899994556, ((1, 1), 'east'): 6.471780716186039, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999743, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 415

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 9, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 11, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 12, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 13, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 14, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 15, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 16, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 17, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 18, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 19, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 20, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 21, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 415 COMPLETE: RETURN WAS 1.09418989132

TOTAL STEPS: 2824, EPISODE STEPS: 22
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289996048203751, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314369543359147, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.471780716186039, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999994, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 416

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 416 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 2831, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289996048203751, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.471780716186039, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999994, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 417

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 417 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 2841, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289996048203751, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.471780716186039, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999996, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 418

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 418 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2849, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289996048203751, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.471780716186039, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999996, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 419

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 419 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2853, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289998024101875, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.471780716186039, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999996, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 420

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 420 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2856, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289998024101875, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.471780716186039, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999996, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 421

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 421 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2861, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999012050936, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.516389468938863, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999996, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 422

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 422 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2866, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999506025467, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999996, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 423

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 423 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2870, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 2.160305007227359, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999996, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 424

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 424 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2882, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 425

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 425 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 2887, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999993193, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 426

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 426 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 2899, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 427

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 427 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2901, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 428

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 428 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2903, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899964399359, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 429

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 429 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2909, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899982199678, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 430

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 430 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2917, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899982199678, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 431

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 431 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 2925, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899982199678, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099992021946274, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 432

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 432 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2928, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899982199678, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 433

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 433 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2934, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314389771679572, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 434

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 434 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2943, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314399885839785, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 435

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 435 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2946, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314399885839785, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 436

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 436 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 2948, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999753012733, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314399885839785, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 437

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 437 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2952, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999876506366, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314399885839785, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 438

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 438 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 2961, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 3.823543842958057, ((0, 2), 'west'): 3.874204889998351, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999876506366, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314399885839785, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 3.507791994853739, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 439

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 6, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 7, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 9, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 10, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 439 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 2977, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.238225221603342, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999876506366, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099996010973136, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.080349297551182, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 440

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 440 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2980, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.238225221603342, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999876506366, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099998005486569, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.080349297551182, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 441

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 441 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2984, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.238225221603342, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999938253182, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099998005486569, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.080349297551182, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 442

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 442 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 2988, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.238225221603342, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099998005486569, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.080349297551182, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 443

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 443 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 2991, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.238225221603342, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899991099839, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.080349297551182, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 444

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.909091
--------------------------------
EPISODE 444 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 2997, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.238225221603342, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.874204889999998, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.2324885536136785, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.080349297551182, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Action 'north' at state (0, 2) is not optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.909091
################################

BEGINNING EPISODE: 445

Current Policy Convergence Ratio: 0.909091
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.909091
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 445 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 3012, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.445565910925984, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.7685803268068385, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 446

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 446 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3017, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.445565910925984, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 3.7685803268068385, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 447

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 447 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 3030, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.445565910925984, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.0366262134034185, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 448

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 448 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3032, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.445565910925984, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.0366262134034185, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 449

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 449 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 3045, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.445565910925984, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314404942919891, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 450

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 450 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 3054, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.169896222498473, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.445565910925984, ((0, 2), 'west'): 3.8742048899995862, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999996912659, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314407471459944, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.538694289892353, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 451

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 451 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3064, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.527360541700795, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999984563295, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314407471459944, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.549847131053142, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 452

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 452 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3066, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.527360541700795, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999984563295, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314407471459944, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.549847131053142, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 453

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 453 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3073, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.527360541700795, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999984563295, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314408735729971, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.549847131053142, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 454

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 454 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3075, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.527360541700795, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999984563295, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314408735729971, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.549847131053142, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 455

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 455 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3077, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.527360541700795, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999984563295, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314408735729971, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899995549918, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.549847131053142, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 456

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 456 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3083, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.527360541700795, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999984563295, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314408735729971, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.549847131053142, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 457

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 457 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3086, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.527360541700795, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999984563295, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314408735729971, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.549847131053142, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 458

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 458 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3092, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.711111479824312, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999992281647, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314408735729971, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.555423558580054, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 459

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 459 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3099, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.711111479824312, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999992281647, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314408735729971, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999996595, ((1, 1), 'east'): 6.555423558580054, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 460

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 460 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 3108, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.711111479824312, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999992281647, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409367864984, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.555423558580054, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 461

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 461 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3110, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.711111479824312, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999992281647, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409367864984, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.555423558580054, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 462

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 462 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3114, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.711111479824312, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999992281647, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409367864984, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.555423558580054, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 463

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 463 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3117, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.711111479824312, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999992281647, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409367864984, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.555423558580054, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 464

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 464 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3122, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.711111479824312, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.549236255587305, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999992281647, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409367864984, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 3.9376071049166916, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.555423558580054, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.366627948899904, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 465

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 465 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3132, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.80549634127318, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999996140823, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409367864984, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.558211775816769, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.753314140370892, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 466

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 466 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3139, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.80549634127318, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999996140823, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999002743285, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.558211775816769, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.753314140370892, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 467

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 467 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3142, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.80549634127318, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999996140823, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999501371641, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.558211775816769, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.753314140370892, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 468

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 468 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3149, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.853943469754136, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999807041, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999501371641, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999998296, ((1, 1), 'east'): 6.559605886171755, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.989130423758377, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 469

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 469 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3159, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.853943469754136, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999807041, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999501371641, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.559605886171755, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.989130423758377, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 470

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 470 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3163, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.853943469754136, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999035205, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999501371641, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.559605886171755, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.989130423758377, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 471

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 471 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3166, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.853943469754136, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999035205, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999501371641, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.559605886171755, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.989130423758377, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 472

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 472 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3169, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.853943469754136, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999035205, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.559605886171755, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.989130423758377, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 473

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 473 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3175, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.878794383654357, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999517601, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56030294265172, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 4.989130423758377, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 474

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 474 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3182, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2899999997588, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56065147110878, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 475

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 475 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3186, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2899999997588, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56065147110878, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 476

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 476 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3193, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2899999997588, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899997774958, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56065147110878, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 477

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 477 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3199, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.2899999997588, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899998887478, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56065147110878, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 478

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 478 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3204, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999879399, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899998887478, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56082573544585, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 479

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 479 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3210, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999879399, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899998887478, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56082573544585, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 480

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 480 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3214, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999939699, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899998887478, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56082573544585, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 481

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 481 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3220, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999939699, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409683932491, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999147, ((1, 1), 'east'): 6.56082573544585, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.1706491567017085, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 482

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 482 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 3233, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999939699, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409841966244, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999572, ((1, 1), 'east'): 6.56082573544585, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 483

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 483 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 3242, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999939699, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999572, ((1, 1), 'east'): 6.56082573544585, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 484

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 484 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3248, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.891533516020453, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 4.992309312778237, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999939699, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.148881784629912, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999572, ((1, 1), 'east'): 6.56082573544585, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.140022684523649, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 485

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 485 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 3259, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999969849, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999572, ((1, 1), 'east'): 6.56091286769579, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 486

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 486 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3269, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999969849, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.099999750685821, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.56091286769579, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 487

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 487 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3272, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999969849, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.56091286769579, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 488

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 488 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3276, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999984923, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.56091286769579, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 489

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 489 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3286, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999984923, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.56091286769579, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 490

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 490 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3291, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999246, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560956433841111, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 491

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 491 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3294, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999246, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560956433841111, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 492

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 492 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3297, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999246, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560956433841111, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 493

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 493 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3299, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.898138338960859, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999246, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560956433841111, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 494

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 494 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3305, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.90149956470893, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999623, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560978216917162, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 495

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 495 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3307, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.90149956470893, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999623, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560978216917162, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 496

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 496 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3309, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.90149956470893, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999623, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560978216917162, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 497

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 497 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3315, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.903189979967188, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999998114, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 3.4867843722413614, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.5609891084568845, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 498

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 498 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3322, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999056, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.5609945542275945, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 499

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 499 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3327, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 500

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 500 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3330, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 501

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 501 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3333, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 502

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 502 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3336, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 503

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 503 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3339, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 504

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 504 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3343, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 505

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 505 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3348, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440992098312, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 506

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 506 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 3356, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999527, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560997277113373, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 507

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 507 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3361, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999763, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 508

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 508 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3365, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 509

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 509 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 3373, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 510

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 510 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3375, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 511

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 511 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3377, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 512

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 512 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3379, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999443739, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 513

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 513 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3385, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 514

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 514 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3387, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 515

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 515 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 3395, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 516

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 516 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3398, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 517

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 517 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3402, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904040088789191, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999988, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.399827677105915, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560998638556473, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 518

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 518 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3409, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904469431745008, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999939, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999319278183, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 519

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 519 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3415, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.237660628350854, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 520

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 520 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 3427, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 521

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 521 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3430, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.09999987534291, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 522

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 522 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3433, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 523

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 523 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3440, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 524

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 524 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3444, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 525

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 525 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3449, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 526

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 526 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3451, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999721868, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 527

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 527 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3457, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 528

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 528 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3460, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999969, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999659639064, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 529

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 529 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3465, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999983, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999829819519, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 530

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 530 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3468, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999983, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999829819519, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 531

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 531 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3474, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904684409547687, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.147344738598322, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999983, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999829819519, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 532

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 532 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3481, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999999, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999914909752, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 533

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 533 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3485, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.28999999999999, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999914909752, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 534

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 534 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3490, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999994, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999957454872, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 535

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 535 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3493, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999994, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999957454872, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 536

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 536 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3497, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999994, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999957454872, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 537

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 537 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3502, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999994, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999957454872, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 538

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 538 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3507, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999994, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999860934, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999957454872, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 539

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 539 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3513, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904792128192627, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.2307803535956205, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999994, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 4.856731878508093, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.320980083065162, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999957454872, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.282085479431563, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 540

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 540 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 3524, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999996, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 541

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 541 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3526, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999996, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 542

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 542 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3528, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999996, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 543

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 543 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3532, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 544

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 544 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3537, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 545

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 545 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3541, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 546

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 546 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3545, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440996049156, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.271166364175427, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 547

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 547 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 3556, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904873003330195, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.272546634484492, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409980245779, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.085546659481999, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.514341200650611, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999978727433, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.298199197402464, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 548

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 548 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 3567, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.9048932364735665, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.293466168740833, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409980245779, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.199972251182599, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.629816585843327, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999989363715, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 549

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 549 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 3576, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.9048932364735665, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.293466168740833, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.199972251182599, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.629816585843327, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999989363715, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 550

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 550 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3579, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.9048932364735665, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.293466168740833, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.199972251182599, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.629816585843327, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999989363715, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 551

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 551 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3581, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.9048932364735665, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.293466168740833, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.199972251182599, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.629816585843327, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999989363715, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 552

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 552 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3585, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.9048932364735665, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.293466168740833, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.199972251182599, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.629816585843327, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999989363715, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 553

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 553 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3588, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.9048932364735665, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.293466168740833, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.199972251182599, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.629816585843327, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999989363715, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 554

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 554 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3592, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.9048932364735665, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.293466168740833, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.199972251182599, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.629816585843327, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999989363715, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 555

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 555 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 3604, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999994681857, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 556

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 556 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3606, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999994681857, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 557

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 557 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3612, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999994681857, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 558

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 558 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3617, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 559

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 559 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3621, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 560

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 560 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3626, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 561

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 561 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3628, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999785, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 562

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 562 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 3641, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999891, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 563

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 563 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3646, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3039350407835215, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.696968068855039, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999891, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 564

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 564 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 3659, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999930466, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 565

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 565 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3665, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 566

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 566 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3670, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 567

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 567 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3674, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899146183122, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999997340927, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 568

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 568 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3680, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 569

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 569 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3682, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 570

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 570 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3686, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 571

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 571 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3688, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 572

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 572 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3691, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 573

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 573 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3694, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 574

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 574 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3696, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 575

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 575 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3702, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409990122888, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 576

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 576 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3709, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 577

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 577 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3715, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 578

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 578 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3717, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999937671456, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 579

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 579 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3720, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999965233, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 580

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 580 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3726, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999982616, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 581

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 581 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3729, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999982616, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 582

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 582 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3735, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999991308, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 583

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 583 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3737, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999991308, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 584

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 584 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3741, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999991308, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 585

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 585 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3747, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 586

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 586 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3750, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 587

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 587 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3756, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 588

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 588 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3758, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.560999998670463, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 589

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 589 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3763, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 590

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 590 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3765, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.287919232087713, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 591

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 591 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 3774, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.296295666043855, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 592

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 592 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3777, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.296295666043855, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 593

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 593 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 3786, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.296295666043855, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 594

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 594 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3790, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.296295666043855, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 595

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 595 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3796, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999968835728, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.296295666043855, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 596

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 596 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3799, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 3.9851587661041608, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.296295666043855, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 597

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 597 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3809, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.296295666043855, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 598

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 598 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 3820, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 599

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 599 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3823, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999445, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 600

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 600 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3833, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 601

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 601 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3843, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 602

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 602 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3845, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 603

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 603 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3849, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 604

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 604 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3853, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899571894978, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3091721361741655, ((0, 2), 'west'): 4.381706844330455, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.285799036694508, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7376114957058935, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.56099999933523, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.306298520142831, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 605

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 17, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 18, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 19, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 20, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 21, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 605 COMPLETE: RETURN WAS 1.09418989132

TOTAL STEPS: 3875, EPISODE STEPS: 22
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999984417863, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 606

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 606 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3878, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999992208932, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 607

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 607 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3880, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999992208932, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 608

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 608 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3886, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999995653, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999992208932, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 609

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 609 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3892, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999992208932, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 610

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 610 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3899, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999992208932, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 611

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 611 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3902, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 612

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 612 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3906, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 613

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 613 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3909, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099950614425, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999971, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 614

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 614 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3919, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 615

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 615 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3922, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 616

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 616 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3924, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 617

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 617 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3926, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999667614, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 618

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 618 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3931, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999833806, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.300483883021927, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 619

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 619 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 3941, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899945963367, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.313755074172588, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.307257162580766, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.770450974468184, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999833806, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.302577991510963, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3133958721686705, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 620

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 620 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 3952, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999916902, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.302577991510963, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 621

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 621 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 3955, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999916902, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.302577991510963, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 622

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 622 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3960, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.56099999995845, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.302577991510963, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 623

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 623 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 3962, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.56099999995845, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.302577991510963, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 624

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 624 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 3974, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.56099999995845, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 625

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 625 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 3981, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.56099999995845, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 626

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 626 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 3986, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999979224, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 627

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 627 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 3992, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999979224, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 628

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 628 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 3996, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999979224, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 629

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 629 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4001, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999979224, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 630

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 630 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4006, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899986378661, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314082524894397, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999989611, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31390291176785, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 631

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 631 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4014, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899993184655, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314246256317596, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 0.0, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999997825, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999994804, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 632

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 632 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4021, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899993184655, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314246256317596, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999994804, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 633

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 633 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4030, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899993184655, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314246256317596, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999994804, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 634

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 634 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4034, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899993184655, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314246256317596, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999994804, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 635

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 635 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4038, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899993184655, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314246256317596, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999994804, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 636

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 636 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4043, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899993184655, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314246256317596, ((0, 2), 'west'): 4.755827576291754, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999994804, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 637

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 637 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4052, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.776073117087316, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409997530721, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 638

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 638 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 4062, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.776073117087316, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 639

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 639 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4064, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.776073117087316, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.7829689999999845, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.303625045755481, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 640

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 640 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 4080, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 641

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 641 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4085, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 642

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 642 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4090, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 643

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 643 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4096, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 644

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 644 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4098, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 645

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 645 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4102, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 646

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 646 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4106, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 647

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 647 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4110, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.9048999965899895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314328125091893, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.776415270611756, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999997401, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 648

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 648 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4118, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 649

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 649 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4122, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 650

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 650 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4126, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304410336438869, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 651

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 651 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 4137, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 652

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 652 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4139, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.9048999999989125, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 653

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 653 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4145, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999456, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 654

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 654 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4149, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999456, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 655

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 655 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4153, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999456, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 656

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 656 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4156, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999456, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 657

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 657 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4162, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 658

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 658 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4164, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 659

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 659 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4167, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 660

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 660 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4174, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899998293825, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.310833569098486, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.5609999999987, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 661

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 661 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4181, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999146327, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999999349, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 662

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 662 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4185, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999146327, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999996104465, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999999349, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 663

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 663 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4188, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999146327, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314369061011442, ((0, 2), 'west'): 4.77948421483501, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999999349, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 664

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 664 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4196, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999728, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 665

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 665 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4202, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999863, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 666

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 666 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4209, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.31440999876536, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999863, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999991, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 667

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 667 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 4223, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999961416, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999863, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999994, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 668

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 668 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4225, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999961416, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999863, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999994, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 669

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 669 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 4236, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999863, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304541218219434, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 670

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 670 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 4247, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999863, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 671

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 671 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4251, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999863, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 672

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 672 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4257, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999572871, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.90489999999993, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999673, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 673

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 673 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4263, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.90489999999993, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 674

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 674 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4265, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.90489999999993, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 675

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 675 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4267, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.90489999999993, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 676

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 676 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4271, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.90489999999993, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 677

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 677 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4276, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.90489999999993, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 678

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 678 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4282, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 679

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 679 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4286, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999996, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 680

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 680 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 4296, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999786288, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314389530121568, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999836, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314156449754323, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 681

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 681 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4305, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.90489999989307, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999998052233, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.5609999999999165, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 682

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 682 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4308, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999989307, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.5609999999999165, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 683

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 683 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4310, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.90489999989307, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.5609999999999165, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 684

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 684 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4313, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999989307, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.5609999999999165, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 685

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 685 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4318, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.90489999989307, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.5609999999999165, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 686

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 686 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4320, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.90489999989307, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.5609999999999165, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 687

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 687 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4323, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999989307, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.5609999999999165, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 688

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 688 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4329, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999957, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 689

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 689 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4332, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999957, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 690

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 690 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4334, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.099999999026117, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999957, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 691

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 691 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4337, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999957, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 692

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 692 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4340, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999957, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 693

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 693 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4345, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999978, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 694

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 694 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4352, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.3144099999807075, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999997, ((1, 1), 'east'): 6.560999999999978, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 695

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 695 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4361, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999990353, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999964, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999978, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 696

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 696 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4367, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.9048999999464975, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999990353, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999982, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.312621783781465, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999978, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 697

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 697 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4374, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999990353, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999982, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304606659109716, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 698

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 698 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 4390, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999998793, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999982, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304663919888712, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 699

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 699 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4396, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999998793, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304663919888712, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 700

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 700 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4399, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999998793, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304663919888712, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 701

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 701 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4404, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999998793, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304663919888712, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 702

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 702 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 4414, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999395, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304663919888712, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 703

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 17, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 18, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 19, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 20, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 21, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 703 COMPLETE: RETURN WAS 1.09418989132

TOTAL STEPS: 4436, EPISODE STEPS: 22
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.781208184872654, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 704

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 704 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 4447, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 705

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 705 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4450, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999875, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 706

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 706 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4455, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999993, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 707

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 707 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4457, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999993, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 708

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 708 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4460, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999991, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999993, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 709

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 709 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4466, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999993, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 710

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 710 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4471, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999993, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 711

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 711 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4476, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999955, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 712

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 712 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4478, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999973239, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313515891866656, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.5609999999999955, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 713

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 713 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4486, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999993306, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 714

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 714 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4488, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999993306, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.09999999951306, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 715

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 715 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4491, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999993306, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999993, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.09999999975653, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 716

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 716 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4500, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999993306, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999995, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.09999999975653, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999996, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314346612294326, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 717

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 717 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4508, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999995, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999994, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.09999999975653, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 718

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 718 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4514, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999995, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.09999999975653, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 719

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 719 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4517, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999995, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 720

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 720 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4523, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999995, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 721

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 721 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4525, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999995, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 722

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 722 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4528, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999995, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 723

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 723 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4536, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 724

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 724 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4544, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 725

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 725 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4549, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999996651, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314399764964614, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.313962945927306, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999997, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 3.1381005828721538, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 726

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 726 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 4559, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999161, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144048824808, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 727

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 727 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4564, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999161, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144048824808, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999878264, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 728

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 728 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4567, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999161, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144048824808, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 729

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 729 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4575, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999161, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144048824808, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 730

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 730 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4580, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999161, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144048824808, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 731

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 731 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4583, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999161, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144048824808, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 732

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 732 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4590, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 733

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 733 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4598, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 734

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 734 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4601, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 735

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 735 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4604, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 736

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 736 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4606, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 737

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 737 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4610, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 738

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 738 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4619, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 739

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 739 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4622, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 740

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 740 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4624, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999996, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 741

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 741 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4630, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.90489999999958, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314407441240023, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304670054972178, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 1.2330400131001429, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 742

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 742 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4639, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671077486088, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 743

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 743 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4644, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671077486088, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 744

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 744 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 4655, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 745

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 745 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4660, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 746

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 746 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4663, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 747

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 747 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4668, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 748

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 748 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4674, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 749

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 749 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4680, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 750

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 750 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4682, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 751

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 751 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4687, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 752

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 752 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4690, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 753

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 753 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4696, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.90489999999979, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314408720619823, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.77965529159723, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.008003355108082, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 754

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 754 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 4706, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 755

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 755 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4708, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 756

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 756 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4713, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 757

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 757 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 4723, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 758

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 758 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4727, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999895, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 759

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 759 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4733, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999946, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 760

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 760 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4735, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999946, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 761

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 761 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4737, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999946, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 762

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 762 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4745, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999946, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 763

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 763 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4749, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999946, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 764

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 764 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4753, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999946, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 765

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 765 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4755, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999946, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409680154814, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314186472962899, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.781311857938032, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314394153069063, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 766

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 766 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 4767, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 5.904899999999973, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409920038667, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 767

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 767 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 4775, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999973, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409920038667, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 768

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 768 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4777, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999973, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409920038667, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 769

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 769 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4779, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999973, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409920038667, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 770

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 770 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4781, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999973, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409920038667, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 771

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 771 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4786, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999973, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409920038667, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 772

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 772 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4793, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 0.0, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 773

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 773 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4799, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 2.9524499999999994, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 774

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 774 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 4809, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 2.9524499999999994, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 775

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 775 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4812, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 2.9524499999999994, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 776

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 776 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4818, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 777

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 777 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4827, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304671588743043, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 778

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 778 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 4841, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467197218576, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 779

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 779 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4843, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999985, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409960019321, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467197218576, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 3.895485601832961, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314402076534508, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 780

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 780 COMPLETE: RETURN WAS 1.85302018885

TOTAL STEPS: 4860, EPISODE STEPS: 17
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999939133, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467206804644, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.339227282925175, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409009566802, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 781

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 781 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4863, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999969565, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467206804644, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.339227282925175, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409009566802, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 782

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 782 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 4876, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999969565, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409009566802, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 783

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 783 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4878, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999969565, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409009566802, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 784

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 784 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4883, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999969565, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409009566802, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 785

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 785 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4887, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999969565, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409009566802, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 786

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 786 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4890, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999984782, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782554570554163, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409009566802, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 787

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 787 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 4901, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999984782, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 788

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 788 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4906, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.9048999999999925, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3142982364814255, ((3, 3), 'east'): 8.099999999984782, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 789

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 789 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4913, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999996, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999984782, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 790

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 790 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4916, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999996, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999992392, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 791

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 791 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 4921, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999996, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999992392, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 792

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 792 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4924, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999996, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 4.428674999999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 793

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 793 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4931, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999996, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.166787499999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 794

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 794 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 4937, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.166787499999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 795

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 795 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4939, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.166787499999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 796

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 796 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 4948, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.166787499999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 2.9524499999999994, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 797

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 797 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 4959, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 798

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 798 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4962, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 799

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 799 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4964, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 800

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 800 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4967, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 801

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 801 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 4970, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3143541182407095, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 802

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 802 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 4977, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314382059120353, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 803

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 803 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4981, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314382059120353, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 804

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 804 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4985, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314382059120353, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 805

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 805 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 4987, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314382059120353, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 806

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 806 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 4991, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409980009654, ((0, 2), 'west'): 4.782083986670404, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314382059120353, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 807

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 807 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 5002, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 808

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 808 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5009, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 809

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 809 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5016, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 810

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 810 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5018, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 811

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 811 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5020, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 812

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 812 COMPLETE: RETURN WAS 1.85302018885

TOTAL STEPS: 5037, EPISODE STEPS: 17
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 813

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 813 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5040, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 814

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 814 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5047, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 815

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 815 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5052, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 816

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 816 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5054, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.535843749999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 817

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 817 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5061, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.720371874999999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 818

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 818 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5067, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409990004826, ((0, 2), 'west'): 4.7827477331741175, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314396029560175, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782761776281426, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409504783398, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 4.521500291436076, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 819

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 819 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 5082, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409995002412, ((0, 2), 'west'): 4.782858362089231, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782865383642885, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409876195848, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.5590500728590175, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 820

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 820 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5088, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409995002412, ((0, 2), 'west'): 4.782858362089231, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782865383642885, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409876195848, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.5590500728590175, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 821

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 821 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5090, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409995002412, ((0, 2), 'west'): 4.782858362089231, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782865383642885, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409876195848, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.5590500728590175, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 822

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 822 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5097, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409997501205, ((0, 2), 'west'): 4.782858362089231, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782865383642885, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409876195848, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.5590500728590175, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 823

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 823 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5100, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409997501205, ((0, 2), 'west'): 4.782858362089231, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782865383642885, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409876195848, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.5590500728590175, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 824

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 824 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5104, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409997501205, ((0, 2), 'west'): 4.782858362089231, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782865383642885, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409876195848, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.5590500728590175, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 825

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 825 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5109, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409997501205, ((0, 2), 'west'): 4.782858362089231, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782865383642885, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.561098132466932, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409876195848, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.5590500728590175, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 826

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 826 COMPLETE: RETURN WAS 1.85302018885

TOTAL STEPS: 5126, EPISODE STEPS: 17
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.812635937499999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 827

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 827 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5133, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 828

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 828 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5136, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 829

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 829 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5140, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 830

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 830 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5144, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999996196, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 831

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 831 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5147, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 832

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 832 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5151, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 833

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 833 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5153, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.3144099993753, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782917191259213, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 834

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 834 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 5162, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999843823, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314403014780087, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782943095489049, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 835

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 835 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 5171, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.31440999992191, ((0, 2), 'west'): 4.782941339678963, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.858767968749999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782943095489049, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672084023219, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.672033565671237, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 836

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 17, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 18, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 19, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 20, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 21, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 22, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 23, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 24, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 836 COMPLETE: RETURN WAS 0.797664430769

TOTAL STEPS: 5196, EPISODE STEPS: 25
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 837

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 837 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5198, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 838

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 838 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5202, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 839

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 839 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 5211, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.099999999998097, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 840

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 840 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5214, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 841

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 841 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5221, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 842

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 842 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5224, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 843

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 843 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5226, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 844

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 844 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5229, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 845

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 845 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5234, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 846

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 846 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 5247, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999960954, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.7829560477269535, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 847

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 847 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 5256, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999990238, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782962523854691, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 848

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 848 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5260, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999990238, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782962523854691, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 849

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 849 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5264, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999990238, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.881833984374999, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782962523854691, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 4.428674999999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 850

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 850 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 5274, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999990238, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.899133496093748, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782962523854691, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 851

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 851 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 5287, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999990238, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 2.6572049999990215, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.899133496093748, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782962523854691, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 852

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 852 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 5298, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999990238, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904179187011717, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782962523854691, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.755235141365098, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409984524479, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 853

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 853 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 5313, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999997558, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904179187011717, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782965761925148, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.779502267665146, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409992262238, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 854

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 854 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5316, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999997558, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904179187011717, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782965761925148, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.779502267665146, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409992262238, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.731975036429509, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 855

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 855 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 5325, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999998778, ((0, 2), 'west'): 4.782955169804341, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904179187011717, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314406507390043, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782965761925148, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409992262238, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.818437518214754, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 856

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 856 COMPLETE: RETURN WAS 2.54186582833

TOTAL STEPS: 5339, EPISODE STEPS: 14
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999388, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904179187011717, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782965761925148, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409998065559, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 857

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 857 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5343, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999388, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904179187011717, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782965761925148, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409998065559, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 858

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 858 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5349, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999388, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782965761925148, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409998065559, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 859

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 859 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 5359, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 860

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 860 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5363, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.09999999999905, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 861

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 861 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5366, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999525, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 862

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 862 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5368, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999525, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 863

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 863 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5371, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999525, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 864

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 864 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5377, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999525, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 865

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 865 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5381, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9045395935058576, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999525, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 866

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 866 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5388, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999525, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 867

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 867 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5391, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 868

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 868 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5395, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 869

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 869 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5398, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.883284379553688, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 870

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 870 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5405, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.899496094888422, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 871

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 871 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5407, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.899496094888422, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 872

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 872 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5413, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.899496094888422, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 873

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 873 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5419, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.899496094888422, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 874

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 874 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5422, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999693, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782967380962298, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672092011609, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.781235633831474, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999516387, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.899496094888422, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 875

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 17, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 18, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 19, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 20, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 21, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 875 COMPLETE: RETURN WAS 1.09418989132

TOTAL STEPS: 5444, EPISODE STEPS: 22
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314408253695021, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.899496094888422, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 876

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 876 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5452, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440912684751, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.902198047444211, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 877

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 877 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 5462, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440912684751, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.902198047444211, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 878

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 878 COMPLETE: RETURN WAS 2.2876792455

TOTAL STEPS: 5477, EPISODE STEPS: 15
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440912684751, ((3, 3), 'east'): 8.099999999999763, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.902198047444211, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 879

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 879 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5480, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440912684751, ((3, 3), 'east'): 8.09999999999988, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.902198047444211, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 880

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 880 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5488, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999988, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.902198047444211, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 881

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 881 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5491, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999988, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.902198047444211, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 882

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 882 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5494, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.099999999999941, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999879095, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.902198047444211, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 883

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 883 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 5503, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.099999999999941, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.903549023722105, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 884

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 884 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5506, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.903549023722105, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 885

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 885 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5508, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.903549023722105, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 886

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 886 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5511, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.903549023722105, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 887

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 887 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5513, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.903549023722105, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 888

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 888 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5520, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.903549023722105, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 889

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 889 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5525, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.903549023722105, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 890

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 890 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5532, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904719796752929, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 891

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 891 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5539, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 892

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 892 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5546, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.166787499999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 893

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 893 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5553, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.535843749999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 894

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 894 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5558, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.535843749999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 895

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 895 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5561, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.535843749999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 896

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 896 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5563, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.535843749999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 897

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 897 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 5573, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.535843749999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 898

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 898 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5577, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999997, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.535843749999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 899

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 899 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5580, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904809898376463, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.099999999999984, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.535843749999999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 900

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 900 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 5593, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999979, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9048549491882305, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.099999999999984, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.812635937499999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999969772, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 901

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 901 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5601, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9048549491882305, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.099999999999984, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.812635937499999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 902

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 902 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5604, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9048549491882305, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999999, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.812635937499999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 903

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 903 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5606, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9048549491882305, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999999, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.812635937499999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 904

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 904 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5608, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 3.9858074999995097, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.9048549491882305, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999999, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672098002902, ((3, 4), 'west'): 5.812635937499999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 905

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 17, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 905 COMPLETE: RETURN WAS 1.66771816997

TOTAL STEPS: 5626, EPISODE STEPS: 18
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904894368648527, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409563423753, ((3, 3), 'east'): 8.09999999999999, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.858767968749999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904224511861052, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 906

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 906 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 5636, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904894368648527, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.09999999999999, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.858767968749999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 907

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 907 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5639, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904894368648527, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999994, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.858767968749999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 908

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 908 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5644, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904894368648527, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999994, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.858767968749999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 909

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 909 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5646, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904894368648527, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999994, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.858767968749999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 910

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 910 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5650, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904894368648527, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999994, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.858767968749999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 911

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 911 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5652, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904894368648527, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999994, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.858767968749999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 912

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 912 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 5661, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999994, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 913

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 913 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5664, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 914

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 914 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5667, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999988, ((0, 2), 'west'): 4.782962084901621, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.99951171875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968797620235, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782535658457782, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.3144099999848855, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8929687500000001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 915

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (0, 2), A: north, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (2, 1), A: north, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 17, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 915 COMPLETE: RETURN WAS 0.166771816997

TOTAL STEPS: 5685, EPISODE STEPS: 18
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999992442, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 916

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 916 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5688, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999992442, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 917

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 917 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5696, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 918

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 918 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5701, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 919

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 919 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5703, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 920

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 920 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5708, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 921

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 921 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5712, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 922

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 922 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5717, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 923

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 923 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5720, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 924

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 924 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5723, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 925

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 925 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5726, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 926

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 926 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5730, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 927

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 927 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5733, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099001451, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782914832307216, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 928

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 928 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 5744, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782941916153607, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 929

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 929 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5747, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782941916153607, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 930

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 930 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5750, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782941916153607, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 931

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 931 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5758, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 932

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 932 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5761, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 933

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 933 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5766, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409781711875, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999622, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 934

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 934 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5774, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.650108749999754, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 935

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 935 COMPLETE: RETURN WAS 3.87420489

TOTAL STEPS: 5784, EPISODE STEPS: 10
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 936

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 936 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5787, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099500724, ((3, 4), 'west'): 5.881833984374999, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 937

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 937 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 5803, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099750361, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 938

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 938 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5808, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099750361, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 939

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 939 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5811, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099750361, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 940

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 940 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5814, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099750361, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 941

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 941 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5819, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099750361, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.782955458076803, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 942

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 3), A: south, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 942 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 5830, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 0.03344572265625001, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 943

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: north, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 943 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5835, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 944

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 944 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5839, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 945

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 945 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5843, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 946

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 946 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5847, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968135612696, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409890855936, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968949405055, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 947

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: north, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 947 COMPLETE: RETURN WAS 3.1381059609

TOTAL STEPS: 5859, EPISODE STEPS: 12
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 948

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 948 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5862, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 949

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 949 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5865, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 950

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 950 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5870, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 951

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 951 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5874, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 952

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 952 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5877, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 953

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 953 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5880, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 954

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 954 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5884, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 955

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 955 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5892, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 956

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 956 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5896, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999998109, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 957

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 957 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 5904, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 958

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 958 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5909, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 959

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 959 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5912, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 960

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 960 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5917, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 961

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 961 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5920, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.893366992187499, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 962

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 962 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5927, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904897184324263, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 963

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 963 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5933, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.999755859375, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.45, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 964

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: south, S': (2, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 2), A: exit, S': TERMINAL_STATE, R: 1

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 964 COMPLETE: RETURN WAS 0.9

TOTAL STEPS: 5935, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.3144099454279665, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999054, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.9047311279652615, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 965

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 1), A: north, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 965 COMPLETE: RETURN WAS 2.82429536481

TOTAL STEPS: 5948, EPISODE STEPS: 13
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 966

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 966 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5954, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 967

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 967 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 5957, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 968

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 968 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5959, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 969

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 969 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 5961, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 970

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 970 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5965, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 971

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 971 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 5969, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 972

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 972 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 5975, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.90489859216213, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 973

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 973 COMPLETE: RETURN WAS 5.31441

TOTAL STEPS: 5982, EPISODE STEPS: 7
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899296081064, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 974

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 974 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 5987, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899296081064, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.30467209987518, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 975

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: south, S': (0, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (1, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 16, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 975 COMPLETE: RETURN WAS 1.85302018885

TOTAL STEPS: 6004, EPISODE STEPS: 17
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899296081064, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 976

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 976 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 6006, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899296081064, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 977

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 977 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 6011, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968567806346, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899296081064, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 978

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: west, S': (0, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 978 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 6019, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899296081064, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 979

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 979 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 6022, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899296081064, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.899133496093748, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 980

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: west, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 980 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 6031, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899648040532, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 981

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 981 COMPLETE: RETURN WAS 5.9049

TOTAL STEPS: 6037, EPISODE STEPS: 6
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 4.982259374999876, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899648040532, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 982

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 3), A: north, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (0, 4), A: west, S': (0, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 11, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 12, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 13, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 14, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 15, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 982 COMPLETE: RETURN WAS 2.05891132095

TOTAL STEPS: 6053, EPISODE STEPS: 16
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 983

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 983 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 6056, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 984

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 984 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 6060, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409972713982, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.31440999999988, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 985

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 985 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 6069, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440998635699, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 986

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 986 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 6073, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440998635699, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 987

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 987 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 6076, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440998635699, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 988

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 988 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 6079, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440998635699, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 989

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 989 COMPLETE: RETURN WAS 6.561

TOTAL STEPS: 6084, EPISODE STEPS: 5
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.31440998635699, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 990

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 2), A: south, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 1), A: west, S': (0, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (0, 1), A: east, S': (1, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (1, 1), A: east, S': (2, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 990 COMPLETE: RETURN WAS 4.3046721

TOTAL STEPS: 6093, EPISODE STEPS: 9
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 991

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 991 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 6097, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 992

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 992 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 6101, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 993

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 993 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 6103, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 994

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 994 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 6107, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.148334687499936, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 995

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 995 COMPLETE: RETURN WAS 4.782969

TOTAL STEPS: 6115, EPISODE STEPS: 8
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.2313723437499675, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 996

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (3, 3), A: east, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 996 COMPLETE: RETURN WAS 8.1

TOTAL STEPS: 6118, EPISODE STEPS: 3
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.2313723437499675, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 997

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 997 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 6120, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.2313723437499675, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 998

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (2, 1), A: east, S': (3, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (3, 1), A: east, S': (4, 1), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (4, 1), A: north, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 998 COMPLETE: RETURN WAS 7.29

TOTAL STEPS: 6124, EPISODE STEPS: 4
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.2313723437499675, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 999

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 999 COMPLETE: RETURN WAS 9.0

TOTAL STEPS: 6126, EPISODE STEPS: 2
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.2313723437499675, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899956005066, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################

BEGINNING EPISODE: 1000

Current Policy Convergence Ratio: 0.954545
Step: 0, S: (0, 4), A: east, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 1, S: (1, 4), A: north, S': (1, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 2, S: (1, 4), A: east, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 3, S: (2, 4), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 4, S: (2, 4), A: south, S': (2, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 5, S: (2, 3), A: north, S': (2, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 6, S: (2, 4), A: east, S': (3, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 7, S: (3, 4), A: east, S': (4, 4), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 8, S: (4, 4), A: south, S': (4, 3), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 9, S: (4, 3), A: south, S': (4, 2), R: 0.0

Current Policy Convergence Ratio: 0.954545
Step: 10, S: (4, 2), A: exit, S': TERMINAL_STATE, R: 10

Current Policy Convergence Ratio: 0.954545
--------------------------------
EPISODE 1000 COMPLETE: RETURN WAS 3.486784401

TOTAL STEPS: 6137, EPISODE STEPS: 11
Learned QValue: {((0, 1), 'east'): 5.904899999999998, ((4, 4), 'south'): 8.099999999999998, ((0, 2), 'south'): 5.314409999999997, ((0, 2), 'west'): 4.782968783903172, ((3, 1), 'west'): 0.5948437500000001, ((2, 0), 'exit'): -7.5, ((2, 1), 'east'): 7.289999999999997, ((4, 0), 'exit'): 0, ((2, 3), 'west'): 2.745794648806241, ((0, 3), 'north'): 4.782968999999998, ((4, 2), 'exit'): 10.0, ((2, 4), 'east'): 6.560999999999998, ((0, 1), 'south'): -4.5, ((2, 3), 'east'): 0, ((0, 2), 'east'): 0.281806668233443, ((3, 3), 'south'): 0.08542968750000002, ((3, 3), 'north'): 0.04613203125000001, ((3, 4), 'north'): 0.05638359375000001, ((4, 4), 'north'): 0.03805892578125001, ((4, 1), 'south'): 0, ((2, 1), 'south'): -2.25, ((2, 2), 'exit'): 0.9998779296875, ((0, 3), 'west'): 0.15256819771294788, ((2, 4), 'south'): 5.272891171874983, ((4, 4), 'east'): 0.02537261718750001, ((3, 4), 'east'): 7.289999999999997, ((1, 4), 'north'): 5.314409999999997, ((0, 2), 'north'): 4.304672099999998, ((2, 3), 'north'): 5.904899999999998, ((3, 3), 'west'): 0.151875, ((4, 4), 'west'): 0.04613203125000001, ((0, 3), 'east'): 0.022576768990047454, ((1, 0), 'exit'): -8.75, ((3, 1), 'south'): -2.25, ((2, 4), 'north'): 5.904899978002533, ((1, 1), 'south'): -4.5, ((1, 1), 'west'): 5.314409993178494, ((3, 3), 'east'): 8.099999999999998, ((0, 0), 'exit'): -8.75, ((2, 3), 'south'): 0.67489013671875, ((3, 0), 'exit'): -7.5, ((1, 4), 'east'): 5.904899999999998, ((4, 3), 'west'): 0.08542968750000002, ((4, 1), 'north'): 9.0, ((0, 1), 'north'): 4.782968974702527, ((2, 4), 'west'): 0.29043028459396364, ((1, 4), 'west'): 4.782968999999998, ((1, 1), 'east'): 6.560999999999998, ((0, 4), 'north'): 0.02753549765029907, ((2, 1), 'west'): 1.0594937544062741, ((0, 4), 'south'): 4.304672099937589, ((3, 4), 'west'): 5.902016748046874, ((3, 1), 'north'): 0.16516406250000001, ((3, 4), 'south'): 0.08542968750000002, ((1, 4), 'south'): 0.06337946423034668, ((4, 3), 'east'): 0.057665039062500006, ((4, 1), 'west'): 0.0, ((0, 4), 'west'): 4.782968999999998, ((4, 3), 'north'): 3.661722861328124, ((0, 3), 'south'): 4.7829622290384, ((3, 1), 'east'): 8.099999999999998, ((4, 1), 'east'): 0.0, ((0, 1), 'west'): 5.314409999999938, ((0, 4), 'east'): 5.314409999999997, ((1, 1), 'north'): 5.904857781991314, ((2, 1), 'north'): 0.8962646484375001, ((4, 3), 'south'): 9.0}

--------------------------------

Policy at state (0, 0) is optimal
Policy at state (0, 1) is optimal
Policy at state (0, 2) is optimal
Policy at state (0, 3) is optimal
Policy at state (0, 4) is optimal
Policy at state (1, 0) is optimal
Policy at state (1, 1) is optimal
Policy at state (1, 4) is optimal
Policy at state (2, 0) is optimal
Policy at state (2, 1) is optimal
Policy at state (2, 2) is optimal
Action 'north' at state (2, 3) is not optimal
Policy at state (2, 4) is optimal
Policy at state (3, 0) is optimal
Policy at state (3, 1) is optimal
Policy at state (3, 3) is optimal
Policy at state (3, 4) is optimal
Policy at state (4, 0) is optimal
Policy at state (4, 1) is optimal
Policy at state (4, 2) is optimal
Policy at state (4, 3) is optimal
Policy at state (4, 4) is optimal
################################
Current Policy Convergence Ratio: 0.954545
################################


AVERAGE RETURNS FROM START STATE: 6.20317695501


Traceback (most recent call last):
  File "/Users/lguan/env-tensorflow-py2.7/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/Users/lguan/env-tensorflow-py2.7/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/Users/lguan/Documents/Study/Research/Summer 2018/Week5/BerkeleyGridWorld/experiment/expr_launcher_terminal.py", line 90, in <module>
    run_expr()
  File "/Users/lguan/Documents/Study/Research/Summer 2018/Week5/BerkeleyGridWorld/experiment/expr_launcher_terminal.py", line 86, in run_expr
    tamerGridWorld.run_episodes()
  File "BerkeleyGridWorld/gridworld.py", line 756, in run_episodes
    plotRatios(policy_converge_ratios)
  File "BerkeleyGridWorld/gridworld.py", line 782, in plotRatios
    plt.show()
  File "/Users/lguan/env-tensorflow-py2.7/lib/python2.7/site-packages/matplotlib/pyplot.py", line 253, in show
    return _show(*args, **kw)
  File "/Users/lguan/env-tensorflow-py2.7/lib/python2.7/site-packages/matplotlib/backend_bases.py", line 208, in show
    cls.mainloop()
  File "/Users/lguan/env-tensorflow-py2.7/lib/python2.7/site-packages/matplotlib/backends/_backend_tk.py", line 1073, in mainloop
    Tk.mainloop()
  File "/Users/lguan/env-tensorflow-py2.7/lib/python2.7/lib-tk/Tkinter.py", line 419, in mainloop
    _default_root.tk.mainloop(n)
KeyboardInterrupt
